{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"cv-final-2.ipynb","provenance":[],"collapsed_sections":["3sXjqBddPpXA","JN39HbyjPtIq","Qw-Q26MvPwPg","O3rD6lBsPztL"],"authorship_tag":"ABX9TyP9ts79pGnwNZYuoEEm+Pfo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"g4OXwr_Cqios"},"source":["from google.colab import drive\r\n","drive.mount('/gdrive/')\r\n","!ls /gdrive"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vpXVLryXszg5"},"source":["import os\r\n","\r\n","BASE_PATH = '/gdrive/My Drive/Classes/CSE 455/ghabt/'\r\n","if not os.path.exists(BASE_PATH):\r\n","    os.makedirs(BASE_PATH)\r\n","\r\n","os.chdir(BASE_PATH)\r\n","!pwd\r\n","!ls\r\n","DATA_PATH = BASE_PATH + 'cifar/'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jt_leQo7tJNV","executionInfo":{"status":"ok","timestamp":1616047945811,"user_tz":420,"elapsed":293,"user":{"displayName":"Darren Denq","photoUrl":"","userId":"01740121905710545134"}}},"source":["import torch\r\n","import torch.nn as nn\r\n","from torchvision import datasets\r\n","from torchvision import transforms\r\n","import numpy as np\r\n","import os\r\n","import torch.nn.functional as F\r\n","import torch.optim as optim\r\n","import h5py\r\n","import sys\r\n","sys.path.append(BASE_PATH)\r\n","import pt_util"],"execution_count":45,"outputs":[]},{"cell_type":"code","metadata":{"id":"F371NU2ctK4o","executionInfo":{"status":"ok","timestamp":1616047944919,"user_tz":420,"elapsed":1496,"user":{"displayName":"Darren Denq","photoUrl":"","userId":"01740121905710545134"}}},"source":["import time\r\n","def train(model, device, train_loader, optimizer, epoch, log_interval):\r\n","    model.train()\r\n","    losses = []\r\n","    for batch_idx, (data, label) in enumerate(train_loader):\r\n","        data, label = data.to(device), label.to(device)\r\n","        optimizer.zero_grad()\r\n","        output = model(data)\r\n","        loss = model.loss(output, label)\r\n","        losses.append(loss.item())\r\n","        loss.backward()\r\n","        optimizer.step()\r\n","        if batch_idx % log_interval == 0:\r\n","            print('{} Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\r\n","                time.ctime(time.time()),\r\n","                epoch, batch_idx * len(data), len(train_loader.dataset),\r\n","                100. * batch_idx / len(train_loader), loss.item()))\r\n","    return np.mean(losses)\r\n","\r\n","def test(model, device, test_loader, log_interval=None):\r\n","    model.eval()\r\n","    test_loss = 0\r\n","    correct = 0\r\n","\r\n","    with torch.no_grad():\r\n","        for batch_idx, (data, label) in enumerate(test_loader):\r\n","            data, label = data.to(device), label.to(device)\r\n","            output = model(data)\r\n","            test_loss_on = model.loss(output, label, reduction='sum').item()\r\n","            test_loss += test_loss_on\r\n","            pred = output.max(1)[1]\r\n","            correct_mask = pred.eq(label.view_as(pred))\r\n","            num_correct = correct_mask.sum().item()\r\n","            correct += num_correct\r\n","            if log_interval is not None and batch_idx % log_interval == 0:\r\n","                print('{} Test: [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\r\n","                    time.ctime(time.time()),\r\n","                    batch_idx * len(data), len(test_loader.dataset),\r\n","                    100. * batch_idx / len(test_loader), test_loss_on))\r\n","\r\n","    test_loss /= len(test_loader.dataset)\r\n","    test_accuracy = 100. * correct / len(test_loader.dataset)\r\n","\r\n","    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\r\n","        test_loss, correct, len(test_loader.dataset), test_accuracy))\r\n","    return test_loss, test_accuracy"],"execution_count":44,"outputs":[]},{"cell_type":"code","metadata":{"id":"FUMAyx5u9u0q"},"source":["data_train = datasets.CIFAR10(root=DATA_PATH, train=True, download=True, transform=transforms.ToTensor())\r\n","data_test = datasets.CIFAR10(root=DATA_PATH, train=False, download=True, transform=transforms.ToTensor())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kW92VOZz-Cna","executionInfo":{"status":"ok","timestamp":1615984842543,"user_tz":420,"elapsed":761,"user":{"displayName":"Darren Denq","photoUrl":"","userId":"01740121905710545134"}}},"source":["def trainAndSaveModel(modelClass):\r\n","    # Play around with these constants, you may find a better setting.\r\n","    BATCH_SIZE = 256\r\n","    TEST_BATCH_SIZE = 10\r\n","    EPOCHS = 50\r\n","    LEARNING_RATE = 0.01\r\n","    MOMENTUM = 0.9\r\n","    USE_CUDA = True\r\n","    SEED = None\r\n","    PRINT_INTERVAL = 250\r\n","    WEIGHT_DECAY = 0.0005\r\n","\r\n","    EXPERIMENT_VERSION = f'{modelClass.__name__}_1' # increment this to start a new experiment\r\n","    LOG_PATH = DATA_PATH + 'logs/' + EXPERIMENT_VERSION + '/'\r\n","\r\n","    # Now the actual training code\r\n","    use_cuda = USE_CUDA and torch.cuda.is_available()\r\n","\r\n","    if SEED is not None:\r\n","      torch.manual_seed(SEED)\r\n","\r\n","    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\r\n","    print('Using device', device)\r\n","    import multiprocessing\r\n","    print('num cpus:', multiprocessing.cpu_count())\r\n","\r\n","    kwargs = {'num_workers': multiprocessing.cpu_count(),\r\n","              'pin_memory': True} if use_cuda else {}\r\n","\r\n","    class_names = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\r\n","\r\n","    train_loader = torch.utils.data.DataLoader(data_train, batch_size=BATCH_SIZE,\r\n","                                              shuffle=True, **kwargs)\r\n","    test_loader = torch.utils.data.DataLoader(data_test, batch_size=TEST_BATCH_SIZE,\r\n","                                              shuffle=False, **kwargs)\r\n","\r\n","    model = modelClass().to(device)\r\n","    optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\r\n","    start_epoch = model.load_last_model(LOG_PATH)\r\n","\r\n","    train_losses, test_losses, test_accuracies = pt_util.read_log(LOG_PATH + 'log.pkl', ([], [], []))\r\n","    test_loss, test_accuracy = test(model, device, test_loader)\r\n","\r\n","    test_losses.append((start_epoch, test_loss))\r\n","    test_accuracies.append((start_epoch, test_accuracy))\r\n","\r\n","    try:\r\n","        for epoch in range(start_epoch, EPOCHS + 1):\r\n","            train_loss = train(model, device, train_loader, optimizer, epoch, PRINT_INTERVAL)\r\n","            test_loss, test_accuracy = test(model, device, test_loader)\r\n","            train_losses.append((epoch, train_loss))\r\n","            test_losses.append((epoch, test_loss))\r\n","            test_accuracies.append((epoch, test_accuracy))\r\n","            pt_util.write_log(LOG_PATH + 'log.pkl', (train_losses, test_losses, test_accuracies))\r\n","            model.save_best_model(test_accuracy, LOG_PATH + '%03d.pt' % epoch)\r\n","    except KeyboardInterrupt as ke:\r\n","        print('Interrupted')\r\n","    except:\r\n","        import traceback\r\n","        traceback.print_exc()\r\n","    finally:\r\n","        model.save_model(LOG_PATH + '%03d.pt' % epoch, 0)\r\n","        ep, val = zip(*train_losses)\r\n","        pt_util.plot(ep, val, 'Train loss', 'Epoch', 'Error')\r\n","        ep, val = zip(*test_losses)\r\n","        pt_util.plot(ep, val, 'Test loss', 'Epoch', 'Error')\r\n","        ep, val = zip(*test_accuracies)\r\n","        pt_util.plot(ep, val, 'Test accuracy', 'Epoch', 'Error')"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3sXjqBddPpXA"},"source":["# Network 1"]},{"cell_type":"code","metadata":{"id":"D5z5aw_B_FiB","executionInfo":{"status":"ok","timestamp":1616036345529,"user_tz":420,"elapsed":325,"user":{"displayName":"Darren Denq","photoUrl":"","userId":"01740121905710545134"}}},"source":["# Test set: TODO record avg loss, accuracy for this one\r\n","class CifarNet1(nn.Module):\r\n","    def __init__(self):\r\n","        super(CifarNet1, self).__init__()\r\n","        self.conv1 = nn.Conv2d(in_channels=3, out_channels=4, kernel_size=3, stride=1, padding=1)\r\n","        self.conv2 = nn.Conv2d(in_channels=4, out_channels=4, kernel_size=3, stride=1, padding=1)\r\n","        self.conv3 = nn.Conv2d(in_channels=4, out_channels=8, kernel_size=3, stride=1, padding=1)\r\n","        self.conv4 = nn.Conv2d(in_channels=8, out_channels=8, kernel_size=3, stride=1, padding=1)\r\n","        self.conv5 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=1, padding=1)\r\n","        self.conv6 = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, stride=1, padding=1)\r\n","        self.fc1 = nn.Linear(in_features=16384, out_features=1000)\r\n","        self.fc2 = nn.Linear(in_features=1000, out_features=500)\r\n","        self.fc3 = nn.Linear(in_features=500, out_features=10)\r\n","        self.accuracy = None\r\n","\r\n","    def forward(self, x):\r\n","        x = self.conv1(x)\r\n","        x = F.relu(x)\r\n","        x = self.conv2(x)\r\n","        x = F.relu(x)\r\n","        x = self.conv3(x)\r\n","        x = F.relu(x)\r\n","        x = self.conv4(x)\r\n","        x = F.relu(x)\r\n","        x = self.conv5(x)\r\n","        x = F.relu(x)\r\n","        x = self.conv6(x)\r\n","        x = F.relu(x)\r\n","        x = torch.flatten(x, 1)\r\n","        x = self.fc1(x)\r\n","        x = F.relu(x)\r\n","        x = self.fc2(x)\r\n","        x = F.relu(x)\r\n","        x = self.fc3(x)\r\n","        return x\r\n","\r\n","    def loss(self, prediction, label, reduction='mean'):\r\n","        loss_val = F.cross_entropy(prediction, label.squeeze(), reduction=reduction)\r\n","        return loss_val\r\n","\r\n","    def save_model(self, file_path, num_to_keep=1):\r\n","        pt_util.save(self, file_path, num_to_keep)\r\n","        \r\n","    def save_best_model(self, accuracy, file_path, num_to_keep=1):\r\n","        if self.accuracy == None or accuracy > self.accuracy:\r\n","            self.accuracy = accuracy\r\n","            self.save_model(file_path, num_to_keep)\r\n","\r\n","    def load_model(self, file_path):\r\n","        pt_util.restore(self, file_path)\r\n","\r\n","    def load_last_model(self, dir_path):\r\n","        return pt_util.restore_latest(self, dir_path)\r\n","\r\n","#trainAndSaveModel(CifarNet1)"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JN39HbyjPtIq"},"source":["# Network 2"]},{"cell_type":"code","metadata":{"id":"alg0ARgEFPMN","executionInfo":{"status":"ok","timestamp":1616036346574,"user_tz":420,"elapsed":262,"user":{"displayName":"Darren Denq","photoUrl":"","userId":"01740121905710545134"}}},"source":["# Test set: TODO record avg loss, accuracy for this one\r\n","class CifarNet2(nn.Module):\r\n","    def __init__(self):\r\n","        super(CifarNet2, self).__init__()\r\n","        self.conv1 = nn.Conv2d(in_channels=3, out_channels=4, kernel_size=3, stride=1, padding=1)\r\n","        self.conv2 = nn.Conv2d(in_channels=4, out_channels=4, kernel_size=3, stride=1, padding=1)\r\n","        self.conv3 = nn.Conv2d(in_channels=4, out_channels=8, kernel_size=3, stride=1, padding=1)\r\n","        self.conv4 = nn.Conv2d(in_channels=8, out_channels=8, kernel_size=3, stride=1, padding=1)\r\n","        self.conv5 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=1, padding=1)\r\n","        self.conv6 = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, stride=1, padding=1)\r\n","        self.fc1 = nn.Linear(in_features=16384, out_features=10)\r\n","        self.accuracy = None\r\n","\r\n","    def forward(self, x):\r\n","        x = self.conv1(x)\r\n","        x = F.relu(x)\r\n","        x = self.conv2(x)\r\n","        x = F.relu(x)\r\n","        x = self.conv3(x)\r\n","        x = F.relu(x)\r\n","        x = self.conv4(x)\r\n","        x = F.relu(x)\r\n","        x = self.conv5(x)\r\n","        x = F.relu(x)\r\n","        x = self.conv6(x)\r\n","        x = F.relu(x)\r\n","        x = torch.flatten(x, 1)\r\n","        x = self.fc1(x)\r\n","        return x\r\n","\r\n","    def loss(self, prediction, label, reduction='mean'):\r\n","        loss_val = F.cross_entropy(prediction, label.squeeze(), reduction=reduction)\r\n","        return loss_val\r\n","\r\n","    def save_model(self, file_path, num_to_keep=1):\r\n","        pt_util.save(self, file_path, num_to_keep)\r\n","        \r\n","    def save_best_model(self, accuracy, file_path, num_to_keep=1):\r\n","        if self.accuracy == None or accuracy > self.accuracy:\r\n","            self.accuracy = accuracy\r\n","            self.save_model(file_path, num_to_keep)\r\n","\r\n","    def load_model(self, file_path):\r\n","        pt_util.restore(self, file_path)\r\n","\r\n","    def load_last_model(self, dir_path):\r\n","        return pt_util.restore_latest(self, dir_path)\r\n","\r\n","#trainAndSaveModel(CifarNet2)"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qw-Q26MvPwPg"},"source":["# Network 4"]},{"cell_type":"code","metadata":{"id":"l2TQeEzNH-Pb","executionInfo":{"status":"ok","timestamp":1616036347996,"user_tz":420,"elapsed":301,"user":{"displayName":"Darren Denq","photoUrl":"","userId":"01740121905710545134"}}},"source":["# Test set: TODO record avg loss, accuracy for this one\r\n","class CifarNet4(nn.Module):\r\n","    def __init__(self):\r\n","        super(CifarNet4, self).__init__()\r\n","        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\r\n","        self.conv2 = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, stride=1, padding=1)\r\n","        self.conv3 = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, stride=1, padding=1)\r\n","        self.fc1 = nn.Linear(in_features=16384, out_features=1000)\r\n","        self.fc2 = nn.Linear(in_features=1000, out_features=500)\r\n","        self.fc3 = nn.Linear(in_features=500, out_features=10)\r\n","        self.accuracy = None\r\n","\r\n","    def forward(self, x):\r\n","        x = self.conv1(x)\r\n","        x = F.relu(x)\r\n","        x = self.conv2(x)\r\n","        x = F.relu(x)\r\n","        x = self.conv3(x)\r\n","        x = F.relu(x)\r\n","        x = torch.flatten(x, 1)\r\n","        x = self.fc1(x)\r\n","        x = F.relu(x)\r\n","        x = self.fc2(x)\r\n","        x = F.relu(x)\r\n","        x = self.fc3(x)\r\n","        return x\r\n","\r\n","    def loss(self, prediction, label, reduction='mean'):\r\n","        loss_val = F.cross_entropy(prediction, label.squeeze(), reduction=reduction)\r\n","        return loss_val\r\n","\r\n","    def save_model(self, file_path, num_to_keep=1):\r\n","        pt_util.save(self, file_path, num_to_keep)\r\n","        \r\n","    def save_best_model(self, accuracy, file_path, num_to_keep=1):\r\n","        if self.accuracy == None or accuracy > self.accuracy:\r\n","            self.accuracy = accuracy\r\n","            self.save_model(file_path, num_to_keep)\r\n","\r\n","    def load_model(self, file_path):\r\n","        pt_util.restore(self, file_path)\r\n","\r\n","    def load_last_model(self, dir_path):\r\n","        return pt_util.restore_latest(self, dir_path)\r\n","\r\n","#trainAndSaveModel(CifarNet4)"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O3rD6lBsPztL"},"source":["# Network 5"]},{"cell_type":"code","metadata":{"id":"aqq_PjdNK2rj","executionInfo":{"status":"ok","timestamp":1616036348798,"user_tz":420,"elapsed":259,"user":{"displayName":"Darren Denq","photoUrl":"","userId":"01740121905710545134"}}},"source":["# Test set: TODO record avg loss, accuracy for this one\r\n","class CifarNet5(nn.Module):\r\n","    def __init__(self):\r\n","        super(CifarNet5, self).__init__()\r\n","        self.mp = nn.MaxPool2d(3, stride=2, padding=1)\r\n","        self.dropout = nn.Dropout2d(0.25)\r\n","        self.conv1 = nn.Conv2d(3, 16, 3, stride=2, padding=1)\r\n","        self.conv2 = nn.Conv2d(16, 32, 3, stride=2, padding=1)\r\n","        self.conv2_bn = nn.BatchNorm2d(32)\r\n","        self.conv3 = nn.Conv2d(32, 64, 3, stride=2, padding=1)\r\n","        self.conv4 = nn.Conv2d(64, 128, 3, stride=2, padding=1)\r\n","        self.conv4_bn = nn.BatchNorm2d(128)\r\n","        self.conv5 = nn.Conv2d(128, 256, 3, stride=2, padding=1)\r\n","        self.conv6 = nn.Conv2d(256, 512, 3, stride=2, padding=1)\r\n","        self.conv6_bn = nn.BatchNorm2d(512)\r\n","        self.conv7 = nn.Conv2d(512, 1024, 3, stride=2, padding=1)\r\n","        self.conv8 = nn.Conv2d(1024, 2048, 3, stride=2, padding=1)\r\n","        self.conv8_bn = nn.BatchNorm2d(2048)\r\n","        self.fc1 = nn.Linear(2048, 1024)\r\n","        self.fc2 = nn.Linear(1024, 512)\r\n","        self.fc3 = nn.Linear(512,200)\r\n","        self.accuracy = None\r\n","\r\n","    def forward(self, x):\r\n","        x = self.conv1(x)\r\n","        x = F.relu(x)\r\n","        x = self.conv2(x)\r\n","        x = self.conv2_bn(x)\r\n","        x = F.relu(x)\r\n","        x = self.conv3(x)\r\n","        x = F.relu(x)\r\n","        x = self.conv4(x)\r\n","        x = self.conv4_bn(x)\r\n","        x = self.dropout(x)\r\n","        X = F.relu(x)\r\n","        x = self.conv5(x)\r\n","        x = F.relu(x)\r\n","        x = self.conv6(x)\r\n","        x = self.conv6_bn(x)\r\n","        x= F.relu(x)\r\n","        x = self.conv7(x)\r\n","        x = F.relu(x)\r\n","        x = self.conv8(x)\r\n","        x = self.conv8_bn(x)\r\n","        x = self.mp(x)\r\n","        x = self.dropout(x)\r\n","        x = F.relu(x)\r\n","        x = torch.flatten(x, 1)\r\n","        x = self.fc1(x)\r\n","        x = F.relu(x)\r\n","        x = self.fc2(x)\r\n","        x = F.relu(x)\r\n","        x = self.fc3(x)\r\n","        return x\r\n","\r\n","    def loss(self, prediction, label, reduction='mean'):\r\n","        loss_val = F.cross_entropy(prediction, label.squeeze(), reduction=reduction)\r\n","        return loss_val\r\n","\r\n","    def save_model(self, file_path, num_to_keep=1):\r\n","        pt_util.save(self, file_path, num_to_keep)\r\n","        \r\n","    def save_best_model(self, accuracy, file_path, num_to_keep=1):\r\n","        if self.accuracy == None or accuracy > self.accuracy:\r\n","            self.accuracy = accuracy\r\n","            self.save_model(file_path, num_to_keep)\r\n","\r\n","    def load_model(self, file_path):\r\n","        pt_util.restore(self, file_path)\r\n","\r\n","    def load_last_model(self, dir_path):\r\n","        return pt_util.restore_latest(self, dir_path)\r\n","\r\n","#trainAndSaveModel(CifarNet5)"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3TzyyHqUP25x"},"source":["# Network 6"]},{"cell_type":"code","metadata":{"id":"-23zJnqoou0a","executionInfo":{"status":"ok","timestamp":1616036350067,"user_tz":420,"elapsed":364,"user":{"displayName":"Darren Denq","photoUrl":"","userId":"01740121905710545134"}}},"source":["# Test set: TODO record avg loss, accuracy for this one\r\n","class CifarNet6(nn.Module):\r\n","    def __init__(self):\r\n","        super(CifarNet6, self).__init__()\r\n","        self.dropout = nn.Dropout2d(0.25)\r\n","        self.conv1 = nn.Conv2d(3, 16, 3, stride=2, padding=1)\r\n","        self.conv2 = nn.Conv2d(16, 32, 3, stride=2, padding=1)\r\n","        self.conv2_bn = nn.BatchNorm2d(32)\r\n","        self.conv3 = nn.Conv2d(32, 64, 3, stride=2, padding=1)\r\n","        self.conv4 = nn.Conv2d(64, 128, 3, stride=2, padding=1)\r\n","        self.conv4_bn = nn.BatchNorm2d(128)\r\n","        self.conv5 = nn.Conv2d(128, 256, 3, stride=2, padding=1)\r\n","        self.conv6 = nn.Conv2d(256, 512, 3, stride=2, padding=1)\r\n","        self.conv6_bn = nn.BatchNorm2d(512)\r\n","        self.conv7 = nn.Conv2d(512, 1024, 3, stride=2, padding=1)\r\n","        self.conv8 = nn.Conv2d(1024, 2048, 3, stride=2, padding=1)\r\n","        self.conv8_bn = nn.BatchNorm2d(2048)\r\n","        self.fc1 = nn.Linear(2048, 1024)\r\n","        self.fc2 = nn.Linear(1024, 512)\r\n","        self.fc3 = nn.Linear(512,10)\r\n","        self.accuracy = None\r\n","\r\n","    def forward(self, x):\r\n","        x = self.conv1(x)\r\n","        x = F.relu(x)\r\n","        x = self.conv2(x)\r\n","        x = self.conv2_bn(x)\r\n","        x = F.relu(x)\r\n","        x = self.conv3(x)\r\n","        x = F.relu(x)\r\n","        x = self.conv4(x)\r\n","        x = self.conv4_bn(x)\r\n","        x = self.dropout(x)\r\n","        X = F.relu(x)\r\n","        x = self.conv5(x)\r\n","        x = F.relu(x)\r\n","        x = self.conv6(x)\r\n","        x = self.conv6_bn(x)\r\n","        x= F.relu(x)\r\n","        x = self.conv7(x)\r\n","        x = F.relu(x)\r\n","        x = self.conv8(x)\r\n","        x = self.conv8_bn(x)\r\n","        x = self.dropout(x)\r\n","        x = F.relu(x)\r\n","        x = torch.flatten(x, 1)\r\n","        x = self.fc1(x)\r\n","        x = F.relu(x)\r\n","        x = self.fc2(x)\r\n","        x = F.relu(x)\r\n","        x = self.fc3(x)\r\n","        return x\r\n","\r\n","    def loss(self, prediction, label, reduction='mean'):\r\n","        loss_val = F.cross_entropy(prediction, label.squeeze(), reduction=reduction)\r\n","        return loss_val\r\n","\r\n","    def save_model(self, file_path, num_to_keep=1):\r\n","        pt_util.save(self, file_path, num_to_keep)\r\n","        \r\n","    def save_best_model(self, accuracy, file_path, num_to_keep=1):\r\n","        if self.accuracy == None or accuracy > self.accuracy:\r\n","            self.accuracy = accuracy\r\n","            self.save_model(file_path, num_to_keep)\r\n","\r\n","    def load_model(self, file_path):\r\n","        pt_util.restore(self, file_path)\r\n","\r\n","    def load_last_model(self, dir_path):\r\n","        return pt_util.restore_latest(self, dir_path)\r\n","\r\n","#trainAndSaveModel(CifarNet6)"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sjAzc9N9P-mY"},"source":["# Perturbations\r\n","Note: Assumes all models use standard cross entropy for their loss. If not, should go back and add a version of loss to each class that doesn't squeeze the label parameter then change the corresponding calls below to call model.loss()"]},{"cell_type":"code","metadata":{"id":"UGIpdbzwPD-G"},"source":["# Make sure data is unaugmented\r\n","from torchvision.utils import save_image\r\n","#print(data_train)\r\n","#print(str(type(data_train)))\r\n","#print(dir(data_train))\r\n","print(data_train[0])\r\n","for x in range(10):\r\n","  save_image((data_train[x])[0], 'test' + str(x) + '.png')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5KcKGO3XRru_","executionInfo":{"status":"ok","timestamp":1615983532552,"user_tz":420,"elapsed":332,"user":{"displayName":"Darren Denq","photoUrl":"","userId":"01740121905710545134"}}},"source":["def ce_loss(prediction, label, reduction='mean'):\r\n","        loss_val = F.cross_entropy(prediction, label, reduction=reduction)\r\n","        return loss_val\r\n","\r\n","# original version\r\n","# perturbations is a list of zerod tensors (basically like zerod data_train / whatever)\r\n","def pertube_images(model, data, labels, pertubations, eps, lr, epochs):\r\n","    accuracies = list()\r\n","    criterion = nn.NLLLoss()\r\n","    for epoch in range(10):\r\n","        tot = 0\r\n","        corr = 0\r\n","        for i in range(len(data)):\r\n","            model.zero_grad()\r\n","            tot += 1\r\n","            output = model(torch.unsqueeze(data[i][0] + pertubations[i], 0))\r\n","            if torch.argmax(output).item() == data[i][1]:\r\n","                corr += 1\r\n","            loss = ce_loss(output, torch.LongTensor([data[i][1]]))\r\n","            loss.backward()\r\n","            pertubations[i] = torch.clamp(pertubations[i].grad * lr + pertubations[i], -eps, eps).detach().clone()\r\n","            pertubations[i].requires_grad = True\r\n","        accuracies.append(corr / tot)\r\n","    return accuracies\r\n","\r\n","#TODO: momentum?\r\n","def perturb_images(model, data, perturbations, eps, lr, epochs):\r\n","    accuracies = list()\r\n","    criterion = nn.NLLLoss()\r\n","    for epoch in range(10):\r\n","        tot = 0\r\n","        corr = 0\r\n","        for i in range(len(data)):\r\n","            if i % 100 == 0:\r\n","              print(i, end=' ')\r\n","            model.zero_grad()\r\n","            if perturbations[i].grad: # initially will be set to None\r\n","              perturbations[i].grad.zero_()\r\n","            tot += 1\r\n","            output = model(torch.unsqueeze(data[i][0] + perturbations[i], 0))\r\n","            if torch.argmax(output).item() == data[i][1]:\r\n","                corr += 1\r\n","            loss = ce_loss(output, torch.LongTensor([data[i][1]]))\r\n","            loss.backward()\r\n","            perturbations[i] = torch.clamp(perturbations[i].grad * lr + perturbations[i], -eps, eps).detach().clone()\r\n","        print()\r\n","        print('epoch ' + str(epoch) + ' finished: ' + str(corr) + '/' + str(tot))\r\n","        accuracies.append(corr / tot)\r\n","    return accuracies\r\n","\r\n","def perturb_model_dataset(model, dataset, eps, lr, epochs, num_examples, prefix, *argv):\r\n","  p_list = [torch.zeros(*argv) for x in range(len(dataset))]\r\n","  for p in p_list:\r\n","    p.requires_grad = True\r\n","  accuracies = perturb_images(model, dataset, p_list, eps, lr, epochs)\r\n","  print(prefix + \" set perturbation training accuracies (by epoch): \" + str(accuracies))\r\n","  for i in range(num_examples):\r\n","    save_image(dataset[i][0], prefix + '_original_' + str(i) + '.png')\r\n","    save_image(dataset[i][0] + p_list[i], prefix + '_perturbed_' + str(i) + '_' + model_name + '.png')\r\n","  torch.save(p_list, 'perturbation_tensors_' + prefix + '_' + model_name + '.pt')\r\n","\r\n","def perturb_cifar_model(model, train_data, test_data, eps=0.01, lr=0.0001, epochs=10, num_examples=10):\r\n","  model_name = type(model).__name__\r\n","  perturb_model_dataset(model, train_data, eps, lr, epochs, num_examples, 'train', 3, 32, 32)\r\n","  perturb_model_dataset(model, test_data, eps, lr, epochs, num_examples, 'test', 3, 32, 32)\r\n","    \r\n","def perturb_cifar_class(modelClass, train_data, test_data):\r\n","  EXPERIMENT_VERSION = f'{modelClass.__name__}_1' # change this to appropriate experiment\r\n","  LOG_PATH = DATA_PATH + 'logs/' + EXPERIMENT_VERSION + '/'\r\n","  model = modelClass()\r\n","  epoch = model.load_last_model(LOG_PATH)\r\n","  if epoch < 1:\r\n","    print('WARNING: MODEL MAY NOT HAVE BEEN LOADED')\r\n","  print(str(type(model)))\r\n","  perturb_cifar_model(model, train_data, test_data)\r\n","\r\n"],"execution_count":132,"outputs":[]},{"cell_type":"code","metadata":{"id":"0Y3RfUpI3bAb","executionInfo":{"status":"ok","timestamp":1615987069080,"user_tz":420,"elapsed":654,"user":{"displayName":"Darren Denq","photoUrl":"","userId":"01740121905710545134"}}},"source":["from torchvision.utils import save_image\r\n","\r\n","def ce_loss(prediction, label, reduction='mean'):\r\n","        loss_val = F.cross_entropy(prediction, label, reduction=reduction)\r\n","        return loss_val\r\n","\r\n","# NOT SUPPORTED ANYMORE WITH DATALOADER\r\n","# original version\r\n","# perturbations is a list of zerod tensors (basically like zerod data_train / whatever)\r\n","def pertube_images(model, data, labels, pertubations, eps, lr, epochs):\r\n","    accuracies = list()\r\n","    criterion = nn.NLLLoss()\r\n","    for epoch in range(10):\r\n","        tot = 0\r\n","        corr = 0\r\n","        for i in range(len(data)):\r\n","            model.zero_grad()\r\n","            tot += 1\r\n","            output = model(torch.unsqueeze(data[i][0] + pertubations[i], 0))\r\n","            if torch.argmax(output).item() == data[i][1]:\r\n","                corr += 1\r\n","            loss = ce_loss(output, torch.LongTensor([data[i][1]]))\r\n","            loss.backward()\r\n","            pertubations[i] = torch.clamp(pertubations[i].grad * lr + pertubations[i], -eps, eps).detach().clone()\r\n","            pertubations[i].requires_grad = True\r\n","        accuracies.append(corr / tot)\r\n","    return accuracies\r\n","\r\n","#TODO: momentum?\r\n","def perturb_images(device, model, dataloader, perturbations, eps, lr, epochs):\r\n","    accuracies = list()\r\n","    for epoch in range(10):\r\n","        tot = 0\r\n","        corr = 0\r\n","        for batch_idx, (data, label) in enumerate(dataloader):\r\n","            data, label = data.to(device), label.to(device)\r\n","            if batch_idx % 100 == 0:\r\n","              print(batch_idx * data.size(0), end=' ')\r\n","            model.zero_grad()\r\n","            if perturbations[batch_idx].grad != None: # initially will be set to None\r\n","              perturbations[batch_idx].grad.zero_()\r\n","            tot += data.size(0)\r\n","            output = model(data + perturbations[batch_idx])\r\n","            pred = output.max(1)[1]\r\n","            #if batch_idx % 100 == 0:\r\n","            #  print(perturbations[batch_idx][0])\r\n","            #  print('heehoo')\r\n","            #  print(output)\r\n","            #  print(pred)\r\n","            #  print(label)\r\n","            correct_mask = pred.eq(label.view_as(pred))\r\n","            num_correct = correct_mask.sum().item()\r\n","            corr += num_correct\r\n","            loss = model.loss(output, label, reduction='sum')\r\n","            loss.backward()\r\n","            if perturbations[batch_idx].grad != None:\r\n","              perturbations[batch_idx] = torch.clamp(perturbations[batch_idx].grad * lr + perturbations[batch_idx], -eps, eps).detach().clone()\r\n","        print()\r\n","        print('epoch ' + str(epoch) + ' finished: ' + str(corr) + '/' + str(tot))\r\n","        accuracies.append(corr / tot)\r\n","    return accuracies\r\n","\r\n","# argv is dimensions of tensors\r\n","def perturb_model_dataset(device, model, dataloader, eps, lr, epochs, num_examples, prefix, *argv):\r\n","  model_name = type(model).__name__\r\n","  p_list = [torch.zeros(*argv, device=device) for x in range(len(dataloader.dataset) // argv[0])]\r\n","  for p in p_list:\r\n","    p.requires_grad = True\r\n","  accuracies = perturb_images(device, model, dataloader, p_list, eps, lr, epochs)\r\n","  print(prefix + \" set perturbation training accuracies (by epoch): \" + str(accuracies))\r\n","  for i in range(num_examples):\r\n","    og = dataloader.dataset[i][0]\r\n","    p = p_list[i//argv[0]][i%argv[0]].to('cpu')\r\n","    save_image(og, prefix + '_original_' + str(i) + '.png')\r\n","    save_image(og + p, prefix + '_perturbed_' + str(i) + '_' + model_name + '.png')\r\n","  torch.save(p_list, 'perturbation_tensors_' + prefix + '_' + model_name + '.pt')\r\n","\r\n","def perturb_cifar_model(device, model, train_data, test_data, batch_size, eps=0.01, lr=1.0, epochs=10, num_examples=10):\r\n","  perturb_model_dataset(device, model, train_data, eps, lr, epochs, num_examples, 'train', batch_size, 3, 32, 32)\r\n","  perturb_model_dataset(device, model, test_data, eps, lr, epochs, num_examples, 'test', batch_size, 3, 32, 32)\r\n","    \r\n","def perturb_cifar_class(modelClass, train_data, test_data):\r\n","  USE_CUDA = True\r\n","  BATCH_SIZE = 100 # jank means this MUST divide len(train_data) and len(test_data)\r\n","  EXPERIMENT_VERSION = f'{modelClass.__name__}_1' # change this to appropriate experiment\r\n","  LOG_PATH = DATA_PATH + 'logs/' + EXPERIMENT_VERSION + '/'\r\n","  use_cuda = USE_CUDA and torch.cuda.is_available()\r\n","  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\r\n","  print('Using device', device)\r\n","  import multiprocessing\r\n","  print('num cpus:', multiprocessing.cpu_count())\r\n","\r\n","  kwargs = {'num_workers': multiprocessing.cpu_count(),\r\n","            'pin_memory': True} if use_cuda else {}\r\n","\r\n","  train_loader = torch.utils.data.DataLoader(data_train, batch_size=BATCH_SIZE,\r\n","                                            shuffle=False, **kwargs)\r\n","  test_loader = torch.utils.data.DataLoader(data_test, batch_size=BATCH_SIZE,\r\n","                                            shuffle=False, **kwargs)\r\n","  model = modelClass().to(device)\r\n","  epoch = model.load_last_model(LOG_PATH)\r\n","  if epoch < 1:\r\n","    print('WARNING: MODEL MAY NOT HAVE BEEN LOADED')\r\n","  print(str(type(model)))\r\n","  perturb_cifar_model(device, model, train_loader, test_loader, BATCH_SIZE)\r\n","\r\n"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"X_39_dPso90o","executionInfo":{"status":"ok","timestamp":1616046079917,"user_tz":420,"elapsed":610,"user":{"displayName":"Darren Denq","photoUrl":"","userId":"01740121905710545134"}}},"source":["from torchvision.utils import save_image\r\n","from PIL import Image\r\n","from torchvision import transforms\r\n","\r\n","tensorize = transforms.ToTensor()\r\n","imagize = transforms.ToPILImage()\r\n","\r\n","def perturb_images(model, examples, labels, name, prefix='ee', num_examples='10'):\r\n","  BATCH_SIZE = 100\r\n","  lr = 10.\r\n","  eps = 0.01\r\n","  model_name = type(model).__name__\r\n","  criterion = nn.NLLLoss()\r\n","  perturbations = [torch.zeros(BATCH_SIZE, 3, 32, 32).to(device).requires_grad_() for im in range(len(labels))]\r\n","  for epoch in range(3):\r\n","      tot = 0\r\n","      corr = 0\r\n","      for i in range(len(examples)):\r\n","          if i % 10 == 0:\r\n","            print(i, end=' ')\r\n","          model.zero_grad()\r\n","          tot += BATCH_SIZE\r\n","          outputs = model(examples[i] + perturbations[i])\r\n","          for output, label in zip(outputs, labels[i]):\r\n","              if torch.argmax(output).item() == label.item():\r\n","                  corr += 1\r\n","          loss = criterion(outputs, labels[i])\r\n","          loss.backward()\r\n","          perturbations[i] = torch.clamp(perturbations[i].grad * lr + perturbations[i], -eps, eps).detach().clone()\r\n","          perturbations[i].requires_grad = True\r\n","      print()\r\n","      print(corr)\r\n","      print(tot)\r\n","      print(corr / tot)\r\n","  #names = [img_name.rstrip().replace('train', 'train_pertubed_02').replace('test', 'test_pertubed_02') for img_name in open('cifar.test', 'r')]\r\n","  #idx = 0\r\n","  #for example, pertubation in zip(examples, pertubations):\r\n","      #for ex, pert in zip(example, pertubation):\r\n","          #imagize(ex.cpu() + pert.cpu()).save(names[idx], format='PNG')\r\n","          #idx += 1\r\n","  torch.save(perturbations, prefix + '_' + model_name + '_' + name + '_perturbations.pt')\r\n","  counter = 0\r\n","  for i in range(len(examples)):\r\n","    model.zero_grad()\r\n","    outputs_og = model(examples[i])\r\n","    outputs = model(examples[i] + perturbations[i])\r\n","    for example, perturbation, output, output_og, label in zip(examples[i], perturbations[i], outputs, outputs_og, labels[i]):\r\n","      if (torch.argmax(output).item() != label.item()) and (torch.argmax(output_og).item() == label.item()):\r\n","        counter += 1\r\n","        print(counter)\r\n","        excpu = example.cpu()\r\n","        save_image(excpu, prefix + '_' + name + '_' + str(i) + '.png')\r\n","        save_image(excpu + perturbation.cpu(), prefix + '_' + name + '_' + str(i) + '_perturbed_' + model_name + '.png')\r\n","        if counter == num_examples:\r\n","          break\r\n","    if counter == num_examples:\r\n","      break\r\n","\r\n","def generate_model_perturbations(modelClass):\r\n","  USE_CUDA = True\r\n","  BATCH_SIZE = 100 # jank means this MUST divide len(train_data) and len(test_data)\r\n","  EXPERIMENT_VERSION = f'{modelClass.__name__}_1' # change this to appropriate experiment\r\n","  LOG_PATH = DATA_PATH + 'logs/' + EXPERIMENT_VERSION + '/'\r\n","  use_cuda = USE_CUDA and torch.cuda.is_available()\r\n","  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\r\n","  print('Using device', device)\r\n","  model = modelClass().to(device)\r\n","  epoch = model.load_last_model(LOG_PATH)\r\n","  if epoch < 1:\r\n","    print('WARNING: MODEL MAY NOT HAVE BEEN LOADED')\r\n","  print('Perturbing ' + modelClass.__name__)\r\n","  possible_labels = {'airplane' : 0,\r\n","                      'automobile': 1,\r\n","                      'bird': 2,\r\n","                      'cat': 3,\r\n","                      'deer': 4,\r\n","                      'dog': 5,\r\n","                      'frog': 6,\r\n","                      'horse': 7,\r\n","                      'ship': 8,\r\n","                      'truck': 9}\r\n","  train_examples, train_labels = generate_ex_l(device, 'cifar.train', possible_labels)\r\n","  train_examples, test_labels = generate_ex_l(device, 'cifar.test', possible_labels)\r\n","  print('data generated')\r\n","  perturb_images(model, train_examples, train_labels, 'train')\r\n","  perturb_images(model, test_examples, test_labels, 'test')\r\n","\r\n","# file_name should be 'cifar.train' or 'cifar.test'\r\n","def generate_ex_l(device, file_name, possible_labels):\r\n","  i = 0\r\n","  examples = []\r\n","  labels = []\r\n","  batch_examples = []\r\n","  batch_labels = []\r\n","  for example in open(file_name, 'r'):\r\n","      BATCH_SIZE = 100\r\n","      batch_examples.append(tensorize(Image.open(example.rstrip())).to(device))\r\n","      batch_examples[-1].requires_grad = False\r\n","      batch_labels.append(torch.LongTensor([possible_labels[example[example.find('_') + 1 : example.find(\".\")]]]).to(device))\r\n","      if i % BATCH_SIZE == (BATCH_SIZE - 1):\r\n","          examples.append(torch.stack(batch_examples))\r\n","          labels.append(torch.squeeze(torch.stack(batch_labels)))\r\n","          batch_examples = list()\r\n","          batch_labels = list()\r\n","      i += 1\r\n","  return examples, labels\r\n","\r\n","def get_device(USE_CUDA=True):\r\n","  use_cuda = USE_CUDA and torch.cuda.is_available()\r\n","  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\r\n","  print('Using device', device)\r\n","\r\n","def faster_model_perturbations(device, modelClass, train_ex, train_l, test_ex, test_l):\r\n","  BATCH_SIZE = 100 # jank means this MUST divide len(train_data) and len(test_data)\r\n","  EXPERIMENT_VERSION = f'{modelClass.__name__}_1' # change this to appropriate experiment\r\n","  LOG_PATH = DATA_PATH + 'logs/' + EXPERIMENT_VERSION + '/'\r\n","  model = modelClass().to(device)\r\n","  epoch = model.load_last_model(LOG_PATH)\r\n","  if epoch < 1:\r\n","    print('WARNING: MODEL MAY NOT HAVE BEEN LOADED')\r\n","  #perturb_images(model, train_ex, train_l, 'train')\r\n","  perturb_images(model, test_ex, test_l, 'test')\r\n","\r\n","\r\n","# hopefully faster generate\r\n","def faster_ex_l(device, dataset):\r\n","  examples = []\r\n","  labels = []\r\n","  batch_examples = []\r\n","  batch_labels = []\r\n","  for i in range(len(dataset)):\r\n","      if i % 5000 == 0:\r\n","        print(i, end=' ')\r\n","      BATCH_SIZE = 100\r\n","      batch_examples.append(dataset[i][0].to(device))\r\n","      batch_examples[-1].requires_grad = False\r\n","      batch_labels.append(dataset[i][1])\r\n","      if i % BATCH_SIZE == (BATCH_SIZE - 1):\r\n","          examples.append(torch.stack(batch_examples))\r\n","          labels.append(torch.LongTensor(batch_labels).to(device))\r\n","          batch_examples = list()\r\n","          batch_labels = list()\r\n","  print()\r\n","  return examples, labels\r\n"],"execution_count":36,"outputs":[]},{"cell_type":"code","metadata":{"id":"AVy_SkF5NFAa"},"source":["data_train = datasets.CIFAR10(root=DATA_PATH, train=True, download=True, transform=transforms.ToTensor())\r\n","data_test = datasets.CIFAR10(root=DATA_PATH, train=False, download=True, transform=transforms.ToTensor())\r\n","\r\n","device = get_device()\r\n","train_examples, train_labels = faster_ex_l(device, data_train)\r\n","test_examples, test_labels = faster_ex_l(device, data_test)\r\n","\r\n","#generate_model_perturbations(CifarNet1)\r\n","#generate_model_perturbations(CifarNet2)\r\n","#generate_model_perturbations(CifarNet4)\r\n","#generate_model_perturbations(CifarNet5)\r\n","#generate_model_perturbations(CifarNet6)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B4wBVlerfHa_"},"source":["faster_model_perturbations(device, CifarNet6, train_examples, train_labels, test_examples, test_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"znCcc7kL2rnR"},"source":["from torch.utils.data import Dataset\r\n","class AugmentedDataset(Dataset):\r\n","\r\n","    def __init__(self, og_dataset, perturbations, batchsize, device):\r\n","        self.og_dataset = og_dataset\r\n","        self.perturbations = perturbations\r\n","        self.bs = batchsize\r\n","        self.device = device\r\n","\r\n","    def __len__(self):\r\n","        return len(self.og_dataset) * 2\r\n","\r\n","    def __getitem__(self, index):\r\n","        if index < len(self.og_dataset):\r\n","          return self.og_dataset[index]\r\n","        else:\r\n","          nindex = index % (len(self.og_dataset)-1)\r\n","          #return (self.og_dataset[nindex][0].to(self.device) + self.perturbations[nindex//self.bs][nindex%self.bs], self.og_dataset[nindex][1])\r\n","          return ((self.og_dataset[nindex][0] + self.perturbations[nindex//self.bs][nindex%self.bs]).detach().clone(), self.og_dataset[nindex][1])\r\n","\r\n","def augment_model_6(data_train, data_test):\r\n","  BATCH_SIZE = 100\r\n","  TEST_BATCH_SIZE = 10\r\n","  EPOCHS = 55\r\n","  LEARNING_RATE = 0.01\r\n","  MOMENTUM = 0.1\r\n","  USE_CUDA = True\r\n","  SEED = None\r\n","  PRINT_INTERVAL = 250\r\n","  WEIGHT_DECAY = 0.0005\r\n","\r\n","  EXPERIMENT_VERSION = 'CifarNet6' + '_1' # increment this to start a new experiment\r\n","  LOG_PATH = DATA_PATH + 'logs/' + EXPERIMENT_VERSION + '/'\r\n","  \r\n","  # Now the actual training code\r\n","  use_cuda = USE_CUDA and torch.cuda.is_available()\r\n","  if SEED is not None:\r\n","      torch.manual_seed(SEED)\r\n","\r\n","  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\r\n","  print('Using device', device)\r\n","  import multiprocessing\r\n","  print('num cpus:', multiprocessing.cpu_count())\r\n","\r\n","  kwargs = {'num_workers': multiprocessing.cpu_count(),\r\n","            'pin_memory': True} if use_cuda else {}\r\n","\r\n","  class_names = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\r\n","  \r\n","  train_p = torch.load('ee_CifarNet6_train_perturbations.pt')\r\n","  test_p = torch.load('ee_CifarNet6_test_perturbations.pt')\r\n","  #train_p = [t.to(device) for t in torch.load('ee_CifarNet6_train_perturbations.pt')]\r\n","  #test_p = [t.to(device) for t in torch.load('ee_CifarNet6_test_perturbations.pt')]\r\n","  ndata_train = AugmentedDataset(data_train, train_p, 100, device)\r\n","  ndata_test = AugmentedDataset(data_test, test_p, 100, device)\r\n","  train_loader = torch.utils.data.DataLoader(ndata_train, batch_size=BATCH_SIZE,\r\n","                                              shuffle=True, **kwargs)\r\n","  test_loader = torch.utils.data.DataLoader(ndata_test, batch_size=BATCH_SIZE,\r\n","                                              shuffle=False, **kwargs)\r\n","  \r\n","  model = CifarNet6().to(device)\r\n","  optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\r\n","  start_epoch = model.load_last_model(LOG_PATH)\r\n","\r\n","  train_losses, test_losses, test_accuracies = pt_util.read_log(LOG_PATH + 'log.pkl', ([], [], []))\r\n","\r\n","  try:\r\n","      for epoch in range(start_epoch, EPOCHS + 1):\r\n","          train_loss = train(model, device, train_loader, optimizer, epoch, PRINT_INTERVAL)\r\n","          test_loss, test_accuracy = test(model, device, test_loader)\r\n","          train_losses.append((epoch, train_loss))\r\n","          test_losses.append((epoch, test_loss))\r\n","          test_accuracies.append((epoch, test_accuracy))\r\n","          pt_util.write_log(LOG_PATH + 'log.pkl', (train_losses, test_losses, test_accuracies))\r\n","          model.save_best_model(test_accuracy, LOG_PATH + '%03d.pt' % epoch)\r\n","  except KeyboardInterrupt as ke:\r\n","      print('Interrupted')\r\n","  except:\r\n","      import traceback\r\n","      traceback.print_exc()\r\n","  finally:\r\n","      model.save_model(LOG_PATH + '%03d.pt' % epoch, 0)\r\n","      ep, val = zip(*train_losses)\r\n","      pt_util.plot(ep, val, 'Train loss', 'Epoch', 'Error')\r\n","      ep, val = zip(*test_losses)\r\n","      pt_util.plot(ep, val, 'Test loss', 'Epoch', 'Error')\r\n","      ep, val = zip(*test_accuracies)\r\n","      pt_util.plot(ep, val, 'Test accuracy', 'Epoch', 'Error')\r\n","\r\n","augment_model_6(data_train, data_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2mtc4EJyrHAu"},"source":["print('perturbing CifarNet1')\r\n","perturb_cifar_class(CifarNet1, data_train, data_test)\r\n","print('perturbing CifarNet2')\r\n","perturb_cifar_class(CifarNet2, data_train, data_test)\r\n","print('perturbing CifarNet4')\r\n","perturb_cifar_class(CifarNet4, data_train, data_test)\r\n","print('perturbing CifarNet5')\r\n","perturb_cifar_class(CifarNet5, data_train, data_test)\r\n","print('perturbing CifarNet6')\r\n","perturb_cifar_class(CifarNet6, data_train, data_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W4FA6wh_jPzC"},"source":["# misc testing"]},{"cell_type":"code","metadata":{"id":"7BXPzdHdlMpY"},"source":["print(len(data_train))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8FXoASk7gphn"},"source":["pert = torch.zeros(3, 32, 32)\r\n","pert.requires_grad = True\r\n","model.zero_grad()\r\n","output = model(torch.unsqueeze(data_train[0][0] + pert, 0))\r\n","loss = F.cross_entropy(output, torch.LongTensor([6]))\r\n","print(pert.grad)\r\n","loss.backward()\r\n","print(pert.grad)\r\n","pert.grad.zero_()\r\n","output = model(torch.unsqueeze(data_train[0][0] + pert, 0))\r\n","loss = F.cross_entropy(output, torch.LongTensor([6]))\r\n","loss.backward()\r\n","print(pert.grad)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P0-JjIhXVD6T"},"source":["model = CifarNet1()\r\n","print(str(type(data_train[0][1])))\r\n","print(data_train[0][0].shape)\r\n","output = model(torch.unsqueeze(data_train[0][0], 0))\r\n","tlabel = torch.LongTensor([3])\r\n","print(output.shape, tlabel.shape)\r\n","print(str(type(output)))\r\n","print(output.shape)\r\n","print(torch.LongTensor([3]).shape)\r\n","print(str(type(torch.LongTensor([3]))))\r\n","print(output.size(0))\r\n","print(torch.LongTensor([3]).size(0))\r\n","loss = F.cross_entropy(output, tlabel)\r\n","print(loss)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1iocPUitXweF"},"source":["hm_loader = torch.utils.data.DataLoader(data_train, batch_size=1, shuffle=True)\r\n","for batch_idx, (data, label) in enumerate(hm_loader):\r\n","  print(data.shape)\r\n","  #print(data)\r\n","  print(str(type(data)))\r\n","  print(label)\r\n","  print(str(type(label)))\r\n","  print(batch_idx)\r\n","  output = model(data)\r\n","  print(output)\r\n","  print(output.shape)\r\n","  print(label.shape)\r\n","  loss = model.loss(output, label)\r\n","  break"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BIzx9hgAu96M"},"source":["train_loader = torch.utils.data.DataLoader(data_train, batch_size=BATCH_SIZE,\r\n","                                            shuffle=True, **kwargs)\r\n","print(len(data_test))"],"execution_count":null,"outputs":[]}]}