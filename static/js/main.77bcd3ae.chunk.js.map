{"version":3,"sources":["img/previous_work.png","img/455_results.PNG","img/bird_pert.PNG","img/ship_ex.PNG","components/DropzoneIcon.js","components/DropImageCard.js","components/Scorecard.js","components/ModelDemo.js","components/App.js","reportWebVitals.js","index.js","components/utils.js"],"names":["makeStyles","icon","width","height","color","card","position","display","alignItems","justifyContent","marginBottom","canvas","zIndex","input","item","paddingTop","makeSession","theme","root","padding","demoElement","marginTop","submit","background","backgroundSize","border","borderRadius","boxShadow","shiny","animation","useStyles","align","panel","examplecard","left","transform","maxWidth","memetitletext","fontFamily","detailtext","shinybutton","shinypanel","App","classes","Container","className","CssBaseline","Paper","style","textAlign","Typography","variant","gutterBottom","ratio","src","frameBorder","allow","allowFullScreen","href","previous_work","alt","aspectRatio","expr_results","bird_ex","ship_ex","reportWebVitals","onPerfEntry","Function","then","getCLS","getFID","getFCP","getLCP","getTTFB","ReactDOM","render","StrictMode","document","getElementById","model_url","getLabelName","split","map","p","charAt","toUpperCase","slice","join","getImg","labelName","InferenceSession","backendHint","warmupModel","session","a","loadModel","wait","ms","Promise","res","rej","global","setTimeout","imgConfig","maxHeight","cover","crop","crossOrigin","orientation","getImage","url","loadImage","img","fetchImage","setData","current","type","Error","ctx","getContext","drawImage","data","getImageData","console","log"],"mappings":"gTAAe,G,OAAA,IAA0B,2CCA1B,MAA0B,wCCA1B,MAA0B,sCCA1B,MAA0B,oC,kECKvBA,YAAW,CACzBC,KAAM,CACFC,MAAO,MACPC,OAAQ,MACRC,MAAO,U,aCHGJ,YAAW,CACzBK,KAAM,CACJH,MAAO,QACPC,OAAQ,QACRG,SAAU,WACVC,QAAS,OACTC,WAAY,SACZC,eAAgB,SAChBC,aAAc,IAEhBC,OAAQ,CACNT,MAAO,QACPC,OAAQ,QACRS,OAAQ,EACRN,SAAU,YAEZO,MAAO,CACLD,OAAQ,KACRN,SAAU,c,0CCbEN,YAAW,CACzBK,KAAM,CACFF,OAAQ,QAEZW,KAAM,CACFC,WAAY,M,YCHJC,cAEEhB,aAAW,SAACiB,GAAD,MAAY,CACvCC,KAAM,CACJC,QAAS,aAEXC,YAAa,CACXC,UAAW,OACXX,aAAc,OACdR,MAAO,QAEToB,OAAQ,CACNC,WAAY,2CACZC,eAAgB,YAChBC,OAAQ,EACRC,aAAc,EACdC,UAAW,wCACXvB,MAAO,QACPD,OAAQ,GACRgB,QAAS,UAEXS,MAAO,CAELL,WAAY,6DACZC,eAAgB,YAChBK,UAAW,8BACXF,UAAW,yCAEb,sBAAuB,CACtB,KAAM,CACL,sBAAuB,UAExB,MAAO,CACN,sBAAuB,YAExB,OAAQ,CACP,sBAAuB,eC5B3B,IAAMG,EAAY9B,aAAW,SAACiB,GAAD,MAAY,CACvCC,KAAM,CACJa,MAAO,UAETC,MAAO,CACLb,QAAS,YACTE,UAAW,OACXX,aAAc,QAEhBuB,YAAa,CACXd,QAAS,YACTb,SAAU,WACV4B,KAAM,MACNC,UAAW,qBACXC,SAAU,OAEZC,cAAe,CACbC,WAAY,kDAEdC,WAAY,CACVpB,QAAS,YACTE,UAAW,OACXX,aAAc,QAEhB8B,YAAa,CACXjB,WAAY,6DACZC,eAAgB,YAChBK,UAAW,8BACXJ,OAAQ,EACRC,aAAc,EACdC,UAAW,wCACXvB,MAAO,QACPD,OAAQ,GACRgB,QAAS,UAEXsB,WAAY,CACVlB,WAAY,6DACZC,eAAgB,YAChBK,UAAW,+BAEb,sBAAuB,CACtB,KAAM,CACL,sBAAuB,UAExB,MAAO,CACN,sBAAuB,YAExB,OAAQ,CACP,sBAAuB,eAKZ,SAASa,IACtB,IAAMC,EAAUb,IAEhB,OACE,eAACc,EAAA,EAAD,CAAWC,UAAWF,EAAQzB,KAA9B,UACE,cAAC4B,EAAA,EAAD,IACA,eAACC,EAAA,EAAD,CAAOF,UAAS,UAAKF,EAAQX,MAAb,YAAsBW,EAAQF,YAAcO,MAAO,CAAEC,UAAU,UAA/E,UACE,cAACC,EAAA,EAAD,CAAYC,QAAQ,KAApB,+CAGA,cAACD,EAAA,EAAD,CAAYC,QAAQ,KAApB,+FAIF,eAACJ,EAAA,EAAD,CAAOF,UAAWF,EAAQX,MAA1B,UACE,cAACkB,EAAA,EAAD,CAAYC,QAAQ,KAAKH,MAAO,CAAEC,UAAU,UAAYG,cAAY,EAApE,iCAGA,eAACF,EAAA,EAAD,CAAYE,cAAY,EAAxB,uiBAC4hB,8RAD5hB,OAGA,cAACF,EAAA,EAAD,CAAYE,cAAY,EAAxB,87BAIF,cAACL,EAAA,EAAD,CAAOF,UAAWF,EAAQX,MAAOgB,MAAO,CAAEC,UAAU,UAApD,SACE,cAAC,IAAD,CAAaI,MAAM,SAASL,MAAO,CAAEZ,SAAU,MAAOF,KAAM,MAAOC,UAAW,sBAA9E,SACE,wBACEmB,IAAI,qDACJC,YAAY,IACZC,MAAM,2FACNC,iBAAe,QAIrB,eAACV,EAAA,EAAD,CAAOF,UAAWF,EAAQX,MAA1B,UACE,cAACkB,EAAA,EAAD,CAAYC,QAAQ,KAAKH,MAAO,CAAEC,UAAU,UAAYG,cAAY,EAApE,2BAGA,eAACF,EAAA,EAAD,CAAYE,cAAY,EAAxB,2DACgD,mBAAGM,KAAK,mCAAR,0FADhD,8VAGA,cAACd,EAAA,EAAD,CAAWI,MAAO,CAAEC,UAAU,SAAU/C,MAAM,OAA9C,SACI,cAAC,IAAD,CACEoD,IAAKK,EACLC,IAAI,sBACJC,YAAa,MAGnB,eAACX,EAAA,EAAD,CAAYE,cAAY,EAAxB,sCAC2B,mBAAGM,KAAK,uCAAR,8HAD3B,iSAGA,eAACR,EAAA,EAAD,CAAYE,cAAY,EAAxB,oDACyC,mBAAGM,KAAK,8CAAR,2CADzC,UACuI,mBAAGA,KAAK,gDAAR,+BADvI,gBAIF,eAACX,EAAA,EAAD,CAAOF,UAAWF,EAAQX,MAA1B,UACE,cAACkB,EAAA,EAAD,CAAYC,QAAQ,KAAKH,MAAO,CAAEC,UAAU,UAAYG,cAAY,EAApE,+BAGA,eAACL,EAAA,EAAD,CAAOF,UAAWF,EAAQJ,WAA1B,UACE,cAACW,EAAA,EAAD,CAAYC,QAAQ,KAAKH,MAAO,CAAEC,UAAU,UAAYG,cAAY,EAApE,2BAGA,cAACF,EAAA,EAAD,CAAYE,cAAY,EAAxB,ieAGA,cAACF,EAAA,EAAD,CAAYE,cAAY,EAAxB,ifAGA,cAACF,EAAA,EAAD,CAAYE,cAAY,EAAxB,wpBAKF,eAACL,EAAA,EAAD,CAAOF,UAAWF,EAAQJ,WAA1B,UACE,cAACW,EAAA,EAAD,CAAYC,QAAQ,KAAKH,MAAO,CAAEC,UAAU,UAAYG,cAAY,EAApE,uBAGA,cAACF,EAAA,EAAD,CAAYE,cAAY,EAAxB,yDAGA,cAACF,EAAA,EAAD,CAAYE,cAAY,EAAxB,6IAIF,eAACL,EAAA,EAAD,CAAOF,UAAWF,EAAQJ,WAA1B,UACE,cAACW,EAAA,EAAD,CAAYC,QAAQ,KAAKH,MAAO,CAAEC,UAAU,UAAYG,cAAY,EAApE,6BAGA,eAACF,EAAA,EAAD,CAAYE,cAAY,EAAxB,kLACuK,mBAAGM,KAAK,gDAAR,kBADvK,OAGA,cAACR,EAAA,EAAD,CAAYE,cAAY,EAAxB,sGAIF,eAACL,EAAA,EAAD,CAAOF,UAAWF,EAAQJ,WAA1B,UACE,cAACW,EAAA,EAAD,CAAYC,QAAQ,KAAKH,MAAO,CAAEC,UAAU,UAAYG,cAAY,EAApE,qCAGA,cAACR,EAAA,EAAD,CAAWI,MAAO,CAAEC,UAAU,SAAU/C,MAAM,OAA9C,SACI,cAAC,IAAD,CACEoD,IAAKQ,EACLF,IAAI,sBACJC,YAAa,MAGnB,cAACX,EAAA,EAAD,CAAYE,cAAY,EAAxB,o+BAGA,cAACR,EAAA,EAAD,CAAWI,MAAO,CAAEC,UAAU,SAAU/C,MAAM,OAA9C,SACI,cAAC,IAAD,CACEoD,IAAKS,EACLH,IAAI,oCACJC,YAAa,QAGnB,cAACjB,EAAA,EAAD,CAAWI,MAAO,CAAEC,UAAU,SAAU/C,MAAM,OAA9C,SACI,cAAC,IAAD,CACEoD,IAAKU,EACLJ,IAAI,oCACJC,YAAa,YAKvB,eAACd,EAAA,EAAD,CAAOF,UAAWF,EAAQX,MAA1B,UACE,cAACkB,EAAA,EAAD,CAAYC,QAAQ,KAAKH,MAAO,CAAEC,UAAU,UAAYG,cAAY,EAApE,wBAGA,eAACL,EAAA,EAAD,CAAOF,UAAWF,EAAQJ,WAA1B,UACE,cAACW,EAAA,EAAD,CAAYC,QAAQ,KAAKH,MAAO,CAAEC,UAAU,UAAYG,cAAY,EAApE,kCAGA,cAACF,EAAA,EAAD,CAAYE,cAAY,EAAxB,ugBAGA,cAACF,EAAA,EAAD,CAAYE,cAAY,EAAxB,6PAIF,eAACL,EAAA,EAAD,CAAOF,UAAWF,EAAQJ,WAA1B,UACE,cAACW,EAAA,EAAD,CAAYC,QAAQ,KAAKH,MAAO,CAAEC,UAAU,UAAYG,cAAY,EAApE,wBAGA,cAACF,EAAA,EAAD,CAAYE,cAAY,EAAxB,wUAGA,cAACF,EAAA,EAAD,CAAYE,cAAY,EAAxB,qIAGA,cAACF,EAAA,EAAD,CAAYE,cAAY,EAAxB,8KAIF,eAACL,EAAA,EAAD,CAAOF,UAAWF,EAAQJ,WAA1B,UACE,cAACW,EAAA,EAAD,CAAYC,QAAQ,KAAKH,MAAO,CAAEC,UAAU,UAAYG,cAAY,EAApE,kDAGA,cAACF,EAAA,EAAD,CAAYE,cAAY,EAAxB,6KAGA,cAACF,EAAA,EAAD,CAAYE,cAAY,EAAxB,gjBC7OV,IAYea,EAZS,SAAAC,GAClBA,GAAeA,aAAuBC,UACxC,8BAAqBC,MAAK,YAAkD,IAA/CC,EAA8C,EAA9CA,OAAQC,EAAsC,EAAtCA,OAAQC,EAA8B,EAA9BA,OAAQC,EAAsB,EAAtBA,OAAQC,EAAc,EAAdA,QAC3DJ,EAAOH,GACPI,EAAOJ,GACPK,EAAOL,GACPM,EAAON,GACPO,EAAQP,OCDdQ,IAASC,OACP,cAAC,IAAMC,WAAP,UACE,cAAClC,EAAD,MAEFmC,SAASC,eAAe,SAM1Bb,K,6QCVMc,G,YAAY,kFAGLC,EAAe,SAAAnC,GAAS,OAAIA,EAAUoC,MAAM,KAAKC,KAAI,SAAAC,GAChE,OAAOA,EAAEC,OAAO,GAAGC,cAAgBF,EAAEG,MAAM,MAC1CC,KAAK,MAEKC,EAAS,SAACC,GACrB,MAAO,uCAGIzE,EAEJ,WAIL,OAAO,IAAI0E,mBAAiB,CAC1BC,YAAa,W,SAKJC,E,8EAAf,WAA2BC,GAA3B,SAAAC,EAAA,2F,sBAWO,SAAeC,EAAtB,kC,4CAAO,WAAyBF,GAAzB,SAAAC,EAAA,sEAGCD,EAAQE,UAAUhB,GAHnB,uBAICa,EAAYC,GAJb,4C,sBAyBP,IA4BMG,EAAO,SAAAC,GAAE,OAAI,IAAIC,SAAQ,SAACC,EAAKC,GACnCC,EAAOC,YAAW,kBAAMH,MAAOF,OAG3BM,EAAY,CAChBnE,SAAU,IACVoE,UAAW,IACXC,OAAO,EACPC,MAAM,EACN/F,QAAQ,EACRgG,YAAa,YACbC,aAAa,GAGTC,EAAW,SAAAC,GAAG,OAAI,IAAIZ,SAAQ,SAACC,EAAKC,GACxCW,IAAUD,GAAK,SAAAE,GAAG,OAAIb,EAAIa,KAAMT,OAGrBU,EAAU,uCAAG,WAAOH,EAAKnG,EAAQuG,GAApB,mBAAApB,EAAA,yDACnBnF,GAAWA,EAAOwG,QADC,iEAENN,EAASC,GAFH,UAGP,WADXE,EAFkB,QAGhBI,KAHgB,sBAGQ,IAAIC,MAAM,wBAHlB,cAIlBC,EAAM3G,EAAOwG,QAAQI,WAAW,OAClCC,UAAUR,EAAK,EAAG,GALE,UAMlBhB,EAAK,GANa,QAOlByB,EAAOH,EAAII,aAAa,EAAG,EAAG/G,EAAOwG,QAAQjH,MAAOS,EAAOwG,QAAQhH,QACzEwH,QAAQC,IAAI,kBACZD,QAAQC,IAAIjH,EAAOwG,QAAQjH,MAAM,IAAIS,EAAOwG,QAAQhH,QACpDwH,QAAQC,IAAIH,GACZP,EAAQO,GAXgB,4CAAH,4D","file":"static/js/main.77bcd3ae.chunk.js","sourcesContent":["export default __webpack_public_path__ + \"static/media/previous_work.f3c920e6.png\";","export default __webpack_public_path__ + \"static/media/455_results.3eabc746.PNG\";","export default __webpack_public_path__ + \"static/media/bird_pert.31495ae2.PNG\";","export default __webpack_public_path__ + \"static/media/ship_ex.0d0cd767.PNG\";","import React from 'react';\nimport InsertPhotoIcon from '@material-ui/icons/InsertPhoto';\nimport CheckCircleIcon from '@material-ui/icons/CheckCircle';\nimport { makeStyles } from '@material-ui/core/styles';\n\nconst useStyles = makeStyles({\n    icon: {\n        width: '50%',\n        height: '50%',\n        color: 'grey',\n    },\n});\n\nexport default ({fileLoaded, isDragActive}) => {\n    const classes = useStyles();\n    if (fileLoaded) { return null; }\n    if (isDragActive) { return <CheckCircleIcon className={classes.icon} />; }\n    return <InsertPhotoIcon className={classes.icon} />;\n}\n","import React, {useCallback} from 'react';\nimport DropzoneIcon from './DropzoneIcon'\nimport {useDropzone} from 'react-dropzone';\nimport { makeStyles } from '@material-ui/core/styles';\nimport Card from '@material-ui/core/Card';\n\nconst useStyles = makeStyles({\n    card: {\n      width: '299px',\n      height: '299px',\n      position: 'relative',\n      display: 'flex',\n      alignItems: 'center',\n      justifyContent: 'center',\n      marginBottom: 10,\n    },\n    canvas: {\n      width: '299px',\n      height: '299px',\n      zIndex: 0,\n      position: 'absolute',\n    },\n    input: {\n      zIndex: 9999,\n      position: 'absolute',\n    },\n});\n\nexport default function DropImageCard({setFile, canvasRef, fileLoaded}) {\n  const classes = useStyles();\n  const onDrop = useCallback(acceptedFiles => {\n    if (acceptedFiles.length > 1) {\n      return console.log('Can only upload one file at a time');\n    }\n    if (acceptedFiles.length === 0) return;\n    const file = acceptedFiles[0];\n    if (!file.type.startsWith('image')) {\n      return console.log('File must be an image');\n    }\n    setFile(file);\n  }, [setFile])\n  const {getRootProps, getInputProps, isDragActive} = useDropzone({onDrop})\n\n  return (\n    <Card {...getRootProps()} className={classes.card}>\n      <canvas className={classes.canvas} ref={canvasRef} width={'299px'} height={'299px'} />\n      <input alt=\"Image Dropzone\" type=\"image\" className={classes.input} {...getInputProps()} />\n      <DropzoneIcon fileLoaded={fileLoaded} isDragActive={isDragActive} />\n    </Card>\n  )\n}\n","import React from 'react';\nimport Avatar from '@material-ui/core/Avatar';\nimport Card from '@material-ui/core/Card';\nimport Typography from '@material-ui/core/Typography';\nimport List from '@material-ui/core/List';\nimport ListItem from '@material-ui/core/ListItem';\nimport ListItemAvatar from '@material-ui/core/ListItemAvatar';\nimport ListItemText from '@material-ui/core/ListItemText';\nimport ListItemSecondaryAction from '@material-ui/core/ListItemSecondaryAction';\nimport { makeStyles } from '@material-ui/core/styles';\n\nconst useStyles = makeStyles({\n    card: {\n        height: 'auto',\n    },\n    item: {\n        paddingTop: 10,\n    },\n});\n\nexport default function Scorecard({items}) {\n    const classes = useStyles();\n    return <Card className={classes.card}><List dense>\n        {items.map(({avatar, name, percentage}) => {\n            const id = `${name}-${percentage}`\n            return <ListItem key={id} className={classes.item}>\n                <ListItemAvatar>\n                    <Avatar\n                        alt={`image of ${name}`}\n                        src={avatar}\n                    />\n                </ListItemAvatar>\n                <ListItemText id={id} primary={name} />\n                <ListItemSecondaryAction>\n                    <Typography>{percentage}%</Typography>\n                </ListItemSecondaryAction>\n            </ListItem>\n        })}\n    </List></Card>;\n};\n","import React, {useRef, useEffect, useState} from 'react';\nimport Container from '@material-ui/core/Container';\nimport CssBaseline from '@material-ui/core/CssBaseline';\nimport Typography from '@material-ui/core/Typography';\nimport Button from '@material-ui/core/Button';\nimport Grid from '@material-ui/core/Grid';\nimport TextField from '@material-ui/core/TextField';\nimport { makeStyles } from '@material-ui/core/styles';\n\nimport DropImageCard from './DropImageCard'\nimport Predictions from './Predictions'\nimport { fetchImage, makeSession, loadModel, runModel } from './utils'\n\nconst session = makeSession();\n\nconst useStyles = makeStyles((theme) => ({\n  root: {\n    padding: '15px 30px',\n  },\n  demoElement: {\n    marginTop: '10px',\n    marginBottom: '10px',\n    width: '100%',\n  },\n  submit: {\n    background: 'linear-gradient(45deg, #d08771, #c85b85)',\n    backgroundSize: '200% 200%',\n    border: 0,\n    borderRadius: 3,\n    boxShadow: '0 3px 5px 2px rgba(255, 105, 135, .1)',\n    color: 'white',\n    height: 48,\n    padding: '0 30px',\n  },\n  shiny: {\n    // I wonder if I can randomize the color lmao\n    background: 'linear-gradient(45deg, #ee7752, #e73c7e, #23a6d5, #23d5ab)',\n    backgroundSize: '400% 400%',\n    animation: '$gradient 15s ease infinite',\n    boxShadow: '0 3px 5px 2px rgba(255, 105, 135, .5)',\n  },\n  '@keyframes gradient': {\n  \t'0%': {\n  \t\t'background-position': '0% 50%',\n  \t},\n  \t'50%': {\n  \t\t'background-position': '100% 50%',\n  \t},\n  \t'100%': {\n  \t\t'background-position': '0% 50%',\n  \t},\n  }\n}));\n\nexport default function ModelDemo() {\n  const [loaded, setLoaded] = useState(false);\n  const [isLoading, setIsLoading] = useState(false);\n  const startLoadModel = async () => {\n    if (isLoading || loaded) { return; }\n    setIsLoading(true);\n    await loadModel(session);\n    setLoaded(true);\n    setIsLoading(false);\n  }\n\n  const [file, setFile] = useState(null)\n  const canvas = useRef(null)\n  const [imgData, setImgData] = useState(null)\n  useEffect(() => {\n    console.log('file updated');\n    console.log(file);\n    if (file) fetchImage(file, canvas, setImgData);\n  }, [file])\n\n  const [textData, setTextData] = useState(\"\")\n  const handleTextChange = (event) => {\n    setTextData(event.target.value);\n  };\n\n  const [startedRun, setStartedRun] = useState(null);\n  const [outputMap, setOutputMap] = useState(null);\n  const startRunModel = async () => {\n    // if (!loaded || !imgData || !(textData.length>0)) return;\n    // setStartedRun(true);\n    console.log('clicked start button!');\n    console.log('image data: ')\n    console.log(imgData);\n    console.log('text data: '+textData);\n    // runModel(session, imgData, textData, setOutputMap);\n  };\n  useEffect(() => {\n    if (!loaded) return;\n    setStartedRun(false);\n  }, [outputMap, imgData, textData]); // runs when loaded or data changes\n  const outputData = outputMap && outputMap.values().next().value.data;\n\n  const classes = useStyles();\n  return (\n    <Container className={classes.root}>\n      <Grid container spacing={3}>\n        <Grid item xs={4}>\n          <Button className={`${classes.demoElement} ${classes.submit} ${classes.shiny}`} onClick={startRunModel}>TODO use CONSOLE, DELETE LATER</Button>\n          { !loaded && !isLoading && (<Button className={`${classes.demoElement} ${classes.submit}`} onClick={startLoadModel}>Load model (TODO 40 MB)</Button>) }\n          { !loaded && isLoading && (<Button className={`${classes.demoElement} ${classes.submit}`}>Loading model...</Button>) }\n          { loaded && !file && (<Button className={`${classes.demoElement} ${classes.submit}`}>Need to upload image</Button>) }\n          { loaded && file && !imgData && (<Button className={`${classes.demoElement} ${classes.submit}`}>Loading image...</Button>) }\n          { loaded && file && imgData && !(textData.length>0) && (<Button className={`${classes.demoElement} ${classes.submit}`}>Need to add text</Button>) }\n          { loaded && file && imgData && (textData.length>0) && !startedRun && (<Button className={`${classes.demoElement} ${classes.submit} ${classes.shiny}`} onClick={startRunModel}>WHERE SHOULD I POST THIS?</Button>) }\n          { loaded && startedRun && (<Button className={`${classes.demoElement} ${classes.submit}`}>Running model...</Button>) }\n          <Predictions output={outputData} className={classes.demoElement} />\n        </Grid>\n        <Grid item xs={8}>\n          <div style={{ display:'inline-block', position: 'relative', left: '50%', transform: 'translate(-50%, 0)' }}>\n            <DropImageCard setFile={setFile} canvasRef={canvas} fileLoaded={!!file} className={classes.demoElement} />\n          </div>\n          <TextField id=\"outlined-basic\" label=\"Meme Title\" variant=\"outlined\" value={textData} onChange={handleTextChange} className={classes.demoElement} />\n        </Grid>\n      </Grid>\n    </Container>\n  )\n}\n","import React, { useState } from 'react';\nimport Container from '@material-ui/core/Container';\nimport CssBaseline from '@material-ui/core/CssBaseline';\nimport Typography from '@material-ui/core/Typography';\nimport Paper from '@material-ui/core/Paper';\nimport Button from '@material-ui/core/Button';\nimport { makeStyles } from '@material-ui/core/styles';\n\nimport 'react-aspect-ratio/aspect-ratio.css'\nimport AspectRatio from 'react-aspect-ratio';\nimport Carousel from 'react-material-ui-carousel'\nimport Image from 'material-ui-image'\n\nimport \"fontsource-roboto\"\nimport previous_work from './../img/previous_work.png'\nimport expr_results from './../img/455_results.PNG'\nimport bird_ex from './../img/bird_pert.PNG'\nimport ship_ex from './../img/ship_ex.PNG'\n\nimport ModelDemo from './ModelDemo'\n\nconst useStyles = makeStyles((theme) => ({\n  root: {\n    align: 'center',\n  },\n  panel: {\n    padding: '15px 30px',\n    marginTop: '20px',\n    marginBottom: '20px',\n  },\n  examplecard: {\n    padding: '10px 30px',\n    position: 'relative',\n    left: '50%',\n    transform: 'translate(-50%, 0)',\n    maxWidth: '80%',\n  },\n  memetitletext: {\n    fontFamily: 'Comic Sans MS, Comic Sans, Comic Neue, cursive',\n  },\n  detailtext: {\n    padding: '10px 30px',\n    marginTop: '15px',\n    marginBottom: '15px',\n  },\n  shinybutton: {\n    background: 'linear-gradient(45deg, #ee7752, #e73c7e, #23a6d5, #23d5ab)',\n    backgroundSize: '400% 400%',\n    animation: '$gradient 15s ease infinite',\n    border: 0,\n    borderRadius: 3,\n    boxShadow: '0 3px 5px 2px rgba(255, 105, 135, .5)',\n    color: 'white',\n    height: 48,\n    padding: '0 30px',\n  },\n  shinypanel: {\n    background: 'linear-gradient(45deg, #ee7752, #e73c7e, #23a6d5, #23d5ab)',\n    backgroundSize: '400% 400%',\n    animation: '$gradient 15s ease infinite',\n  },\n  '@keyframes gradient': {\n  \t'0%': {\n  \t\t'background-position': '0% 50%',\n  \t},\n  \t'50%': {\n  \t\t'background-position': '100% 50%',\n  \t},\n  \t'100%': {\n  \t\t'background-position': '0% 50%',\n  \t},\n  }\n}));\n\nexport default function App() {\n  const classes = useStyles();\n\n  return (\n    <Container className={classes.root}>\n      <CssBaseline />\n      <Paper className={`${classes.panel} ${classes.shinypanel}`} style={{ textAlign:'center' }}>\n        <Typography variant=\"h1\">\n          Bullying Models with Perturbation\n        </Typography>\n        <Typography variant=\"h4\">\n          Which model architectures stand up the best to adversarial image perturbation.\n        </Typography>\n      </Paper>\n      <Paper className={classes.panel}>\n        <Typography variant=\"h4\" style={{ textAlign:'center' }} gutterBottom>\n          Problem Description\n        </Typography>\n        <Typography gutterBottom>\n          In recent decades, neural networks have had a large role in the area of computer vision. Computer vision has recently garnered increasing international attention as it has been applied to the world of surveillance. As a reaction to this, individuals with privacy concerns have increasingly become more interested in how to trick these networks into labeling images incorrectly. In particular, the most popular attacks involve changing what the model would see in small enough ways to trick the model, but still be identifiable to humans. <b>We tested how resistant various neural network architectures would be to a whitebox adversarial attack via image perturbation. We also tested how well using one round of image perturbation as a data augmentation method works to improve model performance</b>.\n        </Typography>\n        <Typography gutterBottom>\n          Our approach was to investigate a variety of neural network architectures ranging from linear models to the original ResNet model. One of the most common benchmark datasets is the CIFAR-10 dataset, which we used to train and identify the most accurate neural network architectures out of a few broad categories. From here, we devised a way to perturb images via a whitebox adversarial attack on the trained models. This attack consists of taking a model trained on CIFAR-10, and learning how to perturb images by going against the gradient. We then output perturbed images, and then compare how well these models are able to correctly label the images. We found that more complex models tended to perform better on the CIFAR data set, and that as the models increased in complexity, they became increasingly susceptible to our adversarial attack but were better able to learn from it if we trained them using data perturbed in a similar way.\n        </Typography>\n      </Paper>\n      <Paper className={classes.panel} style={{ textAlign:'center' }}>\n        <AspectRatio ratio=\"16 / 9\" style={{ maxWidth: '60%', left: '50%', transform: 'translate(-50%, 0)' }}>\n          <iframe\n            src=\"https://www.youtube-nocookie.com/embed/vdVnnMOTe3Q\" // <!-- TODO this needs to be a 2-3min video covering problem setup, data used, techniques used (for both of those latter need to distinguish what is new and what isnt) -->\n            frameBorder=\"0\"\n            allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\"\n            allowFullScreen>\n          </iframe>\n        </AspectRatio>\n      </Paper>\n      <Paper className={classes.panel}>\n        <Typography variant=\"h4\" style={{ textAlign:'center' }} gutterBottom>\n          Previous Work\n        </Typography>\n        <Typography gutterBottom>\n          Our original project proposal was inspired by <a href=\"https://arxiv.org/abs/2002.02196\">AI-GAN: Attack-Inspired Generation of Adversarial Examples (Bai et al, 2020)</a>. We wanted to create a GAN that would create perturbed adversarial images in response to a CIFAR-10 trained neural network. However, we found that this would require much more work than originally planned (see further discussion below), so our project shifted away from using GANs to instead generating perturbed images with a whitebox attack.\n        </Typography>\n        <Container style={{ textAlign:'center', width:'50%' }}>\n            <Image\n              src={previous_work}\n              alt=\"previous work table\"\n              aspectRatio={3/1}\n            />\n        </Container>\n        <Typography gutterBottom>\n          Another relevant work is <a href=\"https://arxiv.org/pdf/1801.00553.pdf\">Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey (Akhtar and Mian, IEEE Access 2018)]</a> which examined different methods of performing adversarial attacks on deep learning models with a wide range of approaches (image above related). To contrast with this, in our experiment, we use a uniform attack style but primarily vary the architecture of the model being attacked.\n        </Typography>\n        <Typography gutterBottom>\n          For our experiment itself, we used the <a href=\"https://www.cs.toronto.edu/~kriz/cifar.html\">pre-existing CIFAR-10 dataset</a> and a <a href=\"https://pytorch.org/vision/stable/models.html\">pretrained ResNet</a> model.\n        </Typography>\n      </Paper>\n      <Paper className={classes.panel}>\n        <Typography variant=\"h4\" style={{ textAlign:'center' }} gutterBottom>\n          Behind The Scenes\n        </Typography>\n        <Paper className={classes.detailtext}>\n          <Typography variant=\"h5\" style={{ textAlign:'center' }} gutterBottom>\n            Problem Setup\n          </Typography>\n          <Typography gutterBottom>\n            Our first step was to identify and build models. We decided to try four different model architecture types: a linear model, a convolutional model, a convolutional model with batch norm/dropout, and finally a high performance model called ResNet. These architectures were investigated, trained with the CIFAR dataset, and tweaked to improve their accuracy. The most accurate version of each of these architectures was then selected to be used in our attack experiment. \n          </Typography>\n          <Typography gutterBottom>\n            Once we had our trained models, we used them to perturb images by calculating the gradient at the input level and adding it to an input perturbation vector. The input-level gradient allows us to note which features would lead the model to label images the way they did, so we use this to ideally perturb images so that these important features wouldn't show up. Finally, we saved the perturbed images generated from each model to create both a perturbed training and testing dataset.\n          </Typography>\n          <Typography gutterBottom>\n            After generating our perturbed image datasets, we tested them on our architectures. For each architecture, we trained models in two different ways and compared each trained model’s performance on the original and perturbed image datasets. The first way we trained each model type was with 50 epochs on the original CIFAR training data. The second way we trained each model type was with 25 epochs on the combined (original CIFAR and perturbed) training data. These variants were tested on the perturbed testing data and the original testing data. Finally, we compared the difference in accuracy of these two models across architecture groups.\n\n          </Typography>\n        </Paper>\n        <Paper className={classes.detailtext}>\n          <Typography variant=\"h5\" style={{ textAlign:'center' }} gutterBottom>\n            Data Used\n          </Typography>\n          <Typography gutterBottom>\n            The main dataset that we used was CIFAR-10.\n          </Typography>\n          <Typography gutterBottom>\n            The other datasets we used were those of perturbed images generated during the experiment, which were derived from CIFAR-10.\n          </Typography>\n        </Paper>\n        <Paper className={classes.detailtext}>\n          <Typography variant=\"h5\" style={{ textAlign:'center' }} gutterBottom>\n            Techniques Used\n          </Typography>\n          <Typography gutterBottom>\n            We used PyTorch for constructing our models. We also tuned our ResNet model from the pretrained model PyTorch provides. Information on the ResNet model can be found <a href=\"https://pytorch.org/vision/stable/models.html\">here</a>.\n          </Typography>\n          <Typography gutterBottom>\n            Other than those, our model architecture and perturbation code was written ourselves.\n          </Typography>\n        </Paper>\n        <Paper className={classes.detailtext}>\n          <Typography variant=\"h5\" style={{ textAlign:'center' }} gutterBottom>\n            Experiments and Results\n          </Typography>\n          <Container style={{ textAlign:'center', width:'50%' }}>\n              <Image\n                src={expr_results}\n                alt=\"expereiment results\"\n                aspectRatio={4/2}\n              />\n          </Container>\n          <Typography gutterBottom>\n            Overall, we found that more complex models tended to perform better on the CIFAR data set, as expected. One thing that was surprising about this was that as the models increased in complexity, they became increasingly susceptible to our adversarial attack. Interestingly, while the more complex models were less resilient against image perturbation attacks, they did learn to handle perturbed data more adeptly when we trained them on an augmented dataset. For all model architectures we tested, augmenting the input dataset with the perturbed images decreased the final model performance on the original CIFAR test set, but increased performance on the perturbed test set. This could be because the perturbations serve a kind of “specialized” data augmentation to help the model generalize better to images with a small amount of noise. In this sense, training on this augmented data set for a longer period of time may lead to better overall performance in the long run.\n          </Typography>\n          <Container style={{ textAlign:'center', width:'50%' }}>\n              <Image\n                src={bird_ex}\n                alt=\"example of a perturbed bird image\"\n                aspectRatio={1.5/1}\n              />\n          </Container>\n          <Container style={{ textAlign:'center', width:'50%' }}>\n              <Image\n                src={ship_ex}\n                alt=\"example of a perturbed ship image\"\n                aspectRatio={2/1}\n              />\n          </Container>\n        </Paper>\n      </Paper>\n      <Paper className={classes.panel}>\n        <Typography variant=\"h4\" style={{ textAlign:'center' }} gutterBottom>\n          Discussion\n        </Typography>\n        <Paper className={classes.detailtext}>\n          <Typography variant=\"h5\" style={{ textAlign:'center' }} gutterBottom>\n            Problems Encountered\n          </Typography>\n          <Typography gutterBottom>\n            The first problem that we encountered encouraged a shift in our overall project approach. We had originally planned on using a GAN to generate adversarial perturbed images to attack our model, but when we tried this, we noticed how the GAN would create adversarial datasets containing images that don't match their label. Resolving this issue would involve a much larger code base than we believed we could develop, so we had to shift our project to a whitebox adversarial attack without the use of GAN's.\n          </Typography>\n          <Typography gutterBottom>\n            Other than this, most problems we encountered were small bugs or difficulties with PyTorch and Google Colab. In particular, we repeatedly had issues with the models not training at all or having frustratingly long runtimes for training.\n          </Typography>\n        </Paper>\n        <Paper className={classes.detailtext}>\n          <Typography variant=\"h5\" style={{ textAlign:'center' }} gutterBottom>\n            Next Steps\n          </Typography>\n          <Typography gutterBottom>\n            If we had more computational resources, we would explore more neural network architectures and see how those are affected by our adversarial attack. Alternatively, we could build on our current analysis by using visualizations to try to understand what accounts for the difference in impacts of image perturbation.\n          </Typography>\n          <Typography gutterBottom>\n            We would also be interested in further exploring the idea of using GAN's and performing a black box adversarial attack.\n          </Typography>\n          <Typography gutterBottom>\n            Finally, we would be interested in evaluating how well different neural network architectures respond to a more closed-loop GAN-based model training process.\n          </Typography>\n        </Paper>\n        <Paper className={classes.detailtext}>\n          <Typography variant=\"h5\" style={{ textAlign:'center' }} gutterBottom>\n            How our approach differs from others\n          </Typography>\n          <Typography gutterBottom>\n            Our approach was a study of the various components that exist in modern computer vision deep neural networks and how they are affected by imager perturbations. \n          </Typography>\n          <Typography gutterBottom>\n            Compared to other studies, we examined the architecture of the model being attacked instead of the architecture of the attack or method of generating adversarial examples itself. This means we address an aspect of adversarial input attacks that other surveys have not examined in detail, which is overall beneficial. However, as mentioned in our discussion of future steps, it would be very interesting to do a multi-dimensional analysis studying interaction of different attack methods and attacked model architectures together as well.\n          </Typography>\n        </Paper>\n      </Paper>\n    </Container>\n  );\n}\n","const reportWebVitals = onPerfEntry => {\n  if (onPerfEntry && onPerfEntry instanceof Function) {\n    import('web-vitals').then(({ getCLS, getFID, getFCP, getLCP, getTTFB }) => {\n      getCLS(onPerfEntry);\n      getFID(onPerfEntry);\n      getFCP(onPerfEntry);\n      getLCP(onPerfEntry);\n      getTTFB(onPerfEntry);\n    });\n  }\n};\n\nexport default reportWebVitals;\n","import React from 'react';\nimport ReactDOM from 'react-dom';\nimport './index.css';\nimport App from './components/App';\nimport reportWebVitals from './reportWebVitals';\n\nReactDOM.render(\n  <React.StrictMode>\n    <App />\n  </React.StrictMode>,\n  document.getElementById('root')\n);\n\n// If you want to start measuring performance in your app, pass a function\n// to log results (for example: reportWebVitals(console.log))\n// or send to an analytics endpoint. Learn more: https://bit.ly/CRA-vitals\nreportWebVitals();\n","import loadImage from 'blueimp-load-image';\nimport { Tensor, InferenceSession } from 'onnxjs';\nimport ndarray from 'ndarray';\nimport ops from 'ndarray-ops';\n// import model from '../dogs-resnet18.onnx';\n\nconst model_url = 'https://github.com/davidpfahler/react-ml-app/raw/master/src/dogs-resnet18.onnx'\n// const model_url = '../dogs-resnet18.onnx'\n\nexport const getLabelName = className => className.split('_').map(p => {\n  return p.charAt(0).toUpperCase() + p.slice(1)\n}).join(' ')\n\nexport const getImg = (labelName) => {\n  return 'https://i.redd.it/vb4uq6nipk251.jpg'\n}\n\nexport const makeSession = (() => {\n  let _session = null;\n  return () => {\n    if (_session !== null) {\n      return _session;\n    }\n    return new InferenceSession({\n      backendHint: 'webgl'\n    });\n  }\n})()\n\nasync function warmupModel(session) {\n  // TODO does this even make sense to run for our model?\n  // const dims = [1, 3, 299, 299];\n  // const size = dims.reduce((a, b) => a * b);\n  // const warmupTensor = new Tensor(new Float32Array(size), 'float32', dims);\n  // for (let i = 0; i < size; i++) {\n  //     warmupTensor.data[i] = Math.random() * 2.0 - 1.0; // random value [-1.0, 1.0)\n  // }\n  // await session.run([warmupTensor]);\n}\n\nexport async function loadModel(session) {\n  // TODO swap out on deploying?\n  // await session.loadModel(model);\n  await session.loadModel(model_url);\n  await warmupModel(session);\n}\n\nasync function _runModel(session, imgInput, textInput, setOutputMap) {\n  const {\n    width,\n    height\n  } = imgInput;\n  // TODO modify\n  const data = preprocess(imgInput);\n  const imgInputTensor = new Tensor(data, 'float32', [1, 3, width, height]);\n  // await wait(0);\n  const outputMap = await session.run([imgInputTensor]);\n  setOutputMap(outputMap);\n}\n\nexport function runModel(session, imgInput, textInput, setOutputMap) {\n  setTimeout(() => _runModel(session, imgInput, textInput, setOutputMap), 10);\n}\n\n// borrowed from onnx.js example: https://github.com/microsoft/onnxjs/blob/4085b7e61804d093e36af6a456d8c14c329f0a0a/examples/browser/resnet50/index.js#L29\nconst preprocess = input => {\n  // rescale images to 3x256x256 TODO...\n  console.log(input)\n  const {\n    data,\n    width,\n    height\n  } = input\n\n  // data processing\n  const dataTensor = ndarray(new Float32Array(data), [width, height, 4]);\n  const dataProcessedTensor = ndarray(new Float32Array(width * height * 3), [1, 3, width, height]);\n  ops.assign(dataProcessedTensor.pick(0, 0, null, null), dataTensor.pick(null, null, 0));\n  ops.assign(dataProcessedTensor.pick(0, 1, null, null), dataTensor.pick(null, null, 1));\n  ops.assign(dataProcessedTensor.pick(0, 2, null, null), dataTensor.pick(null, null, 2));\n  ops.divseq(dataProcessedTensor, 255);\n  ops.subseq(dataProcessedTensor.pick(0, 0, null, null), 0.485);\n  ops.subseq(dataProcessedTensor.pick(0, 1, null, null), 0.456);\n  ops.subseq(dataProcessedTensor.pick(0, 2, null, null), 0.406);\n  ops.divseq(dataProcessedTensor.pick(0, 0, null, null), 0.229);\n  ops.divseq(dataProcessedTensor.pick(0, 1, null, null), 0.224);\n  ops.divseq(dataProcessedTensor.pick(0, 2, null, null), 0.225);\n\n  console.log(dataProcessedTensor);\n\n  return dataProcessedTensor.data;\n}\n\nconst wait = ms => new Promise((res, rej) => {\n  global.setTimeout(() => res(), ms)\n});\n\nconst imgConfig = {\n  maxWidth: 299,\n  maxHeight: 299,\n  cover: true,\n  crop: true,\n  canvas: true,\n  crossOrigin: 'Anonymous',\n  orientation: true,\n};\n\nconst getImage = url => new Promise((res, rej) => {\n  loadImage(url, img => res(img), imgConfig)\n});\n\nexport const fetchImage = async (url, canvas, setData) => {\n  if (!canvas || !canvas.current) return;\n  const img = await getImage(url);\n  if (img.type === \"error\") throw new Error(\"could not load image\");\n  const ctx = canvas.current.getContext('2d');\n  ctx.drawImage(img, 0, 0);\n  await wait(1);\n  const data = ctx.getImageData(0, 0, canvas.current.width, canvas.current.height);\n  console.log('in fetchImage,');\n  console.log(canvas.current.width+' '+canvas.current.height)\n  console.log(data)\n  setData(data);\n};\n"],"sourceRoot":""}