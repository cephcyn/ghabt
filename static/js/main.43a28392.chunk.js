(this["webpackJsonpganna-have-a-bad-time"]=this["webpackJsonpganna-have-a-bad-time"]||[]).push([[0],{104:function(e,t,a){},155:function(e,t,a){"use strict";a.r(t);var r=a(0),n=a.n(r),i=a(10),o=a.n(i),s=(a(104),a(19)),c=a(202),d=a(205),l=a(60),u=a(163),h=a(73),m=(a(105),a(85)),g=a.n(m),p=(a(106),a(80)),b=a.n(p),x=(a(129),a.p+"static/media/motivational-leo-v1.ef976b8d.png"),w=a.p+"static/media/motivational-leo-v2.7c71dc7b.png",j=(a.p,a.p,a.p,a.p,a.p,a.p,a(15),a(33),a(204),a(203),a(206),a(59),a(82),a(81),a(3));Object(h.a)({icon:{width:"50%",height:"50%",color:"grey"}}),a(86),a(196),Object(h.a)({card:{width:"299px",height:"299px",position:"relative",display:"flex",alignItems:"center",justifyContent:"center",marginBottom:10},canvas:{width:"299px",height:"299px",zIndex:0,position:"absolute"},input:{zIndex:9999,position:"absolute"}});a(207),a(197),a(198),a(199),a(200),a(201),Object(h.a)({card:{height:"auto"},item:{paddingTop:10}});var f=a(34);Object(f.e)(),Object(h.a)((function(e){return{root:{padding:"15px 30px"},demoElement:{marginTop:"10px",marginBottom:"10px",width:"100%"},submit:{background:"linear-gradient(45deg, #d08771, #c85b85)",backgroundSize:"200% 200%",border:0,borderRadius:3,boxShadow:"0 3px 5px 2px rgba(255, 105, 135, .1)",color:"white",height:48,padding:"0 30px"},shiny:{background:"linear-gradient(45deg, #ee7752, #e73c7e, #23a6d5, #23d5ab)",backgroundSize:"400% 400%",animation:"$gradient 15s ease infinite",boxShadow:"0 3px 5px 2px rgba(255, 105, 135, .5)"},"@keyframes gradient":{"0%":{"background-position":"0% 50%"},"50%":{"background-position":"100% 50%"},"100%":{"background-position":"0% 50%"}}}}));var v=Object(h.a)((function(e){return{root:{align:"center"},panel:{padding:"15px 30px",marginTop:"20px",marginBottom:"20px"},examplecard:{padding:"10px 30px",position:"relative",left:"50%",transform:"translate(-50%, 0)",maxWidth:"80%"},memetitletext:{fontFamily:"Comic Sans MS, Comic Sans, Comic Neue, cursive"},detailtext:{padding:"10px 30px",marginTop:"15px",marginBottom:"15px"},shinybutton:{background:"linear-gradient(45deg, #ee7752, #e73c7e, #23a6d5, #23d5ab)",backgroundSize:"400% 400%",animation:"$gradient 15s ease infinite",border:0,borderRadius:3,boxShadow:"0 3px 5px 2px rgba(255, 105, 135, .5)",color:"white",height:48,padding:"0 30px"},shinypanel:{background:"linear-gradient(45deg, #ee7752, #e73c7e, #23a6d5, #23d5ab)",backgroundSize:"400% 400%",animation:"$gradient 15s ease infinite"},"@keyframes gradient":{"0%":{"background-position":"0% 50%"},"50%":{"background-position":"100% 50%"},"100%":{"background-position":"0% 50%"}}}}));function y(){var e=v(),t=Object(r.useState)(x),a=Object(s.a)(t,2),n=a[0],i=a[1];return Object(j.jsxs)(c.a,{className:e.root,children:[Object(j.jsx)(d.a,{}),Object(j.jsxs)(u.a,{className:"".concat(e.panel," ").concat(e.shinypanel),style:{textAlign:"center"},children:[Object(j.jsx)(l.a,{variant:"h1",children:"Bullying Models with Perturbation"}),Object(j.jsx)(l.a,{variant:"h4",children:"Which model architectures stand up the best to adversarial image perturbation."})]}),Object(j.jsxs)(u.a,{className:e.panel,children:[Object(j.jsx)(l.a,{variant:"h4",style:{textAlign:"center"},gutterBottom:!0,children:"Problem Description"}),Object(j.jsxs)(l.a,{gutterBottom:!0,children:["In recent decades, neural networks have had a large role in the area of computer vision. Computer vision has recently garnered increasing international attention as it has been applied to the world of surveillance. As a reaction to this, individuals with privacy concerns have increasingly become more interested in how to trick these networks into labeling images incorrectly. In particular, the most popular attacks involve changing what the model would see in small enough ways to trick the model, but still be identifiable to humans. ",Object(j.jsx)("b",{children:"We tested how resistant various neural network architectures would be to a whitebox adversarial attack via image perturbation. We also tested how well using one round of image perturbation as a data augmentation method works to improve model performance"}),"."]}),Object(j.jsx)(l.a,{gutterBottom:!0,children:"Our approach was to investigate a variety of neural network architectures ranging from linear models to the original ResNet model. One of the most common benchmark datasets is the CIFAR-10 dataset, which we used to train and identify the most accurate neural network architectures out of a few broad categories. From here, we devised a way to perturb images via a whitebox adversarial attack on the trained models. This attack consists of taking a model trained on CIFAR-10, and learning how to perturb images by going against the gradient. We then output perturbed images, and then compare how well these models are able to correctly label the images. We found that TODO"})]}),Object(j.jsx)(u.a,{className:e.panel,style:{textAlign:"center"},children:Object(j.jsx)(g.a,{ratio:"16 / 9",style:{maxWidth:"60%",left:"50%",transform:"translate(-50%, 0)"},children:Object(j.jsx)("iframe",{src:"https://www.youtube-nocookie.com/embed/vdVnnMOTe3Q",frameBorder:"0",allow:"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture",allowFullScreen:!0})})}),Object(j.jsxs)(u.a,{className:e.panel,children:[Object(j.jsx)(l.a,{variant:"h4",style:{textAlign:"center"},gutterBottom:!0,children:"Previous Work"}),Object(j.jsx)(l.a,{})]}),Object(j.jsxs)(u.a,{className:e.panel,children:[Object(j.jsx)(l.a,{variant:"h4",style:{textAlign:"center"},gutterBottom:!0,children:"Behind The Scenes"}),Object(j.jsxs)(u.a,{className:e.detailtext,children:[Object(j.jsx)(l.a,{variant:"h5",style:{textAlign:"center"},gutterBottom:!0,children:"Problem Setup"}),Object(j.jsx)(l.a,{gutterBottom:!0,children:"Our first step was to identify and build models. We decided to try four different model architecture types: a linear model, a convolutional model, a convolutional model with batch norm/dropout, and finally a high performance model called ResNet. These architectures were investigated, trained with the CIFAR dataset, and tweaked to improve their accuracy. The most accurate version of each of these architectures was then selected to be used in our attack experiment."}),Object(j.jsx)(l.a,{gutterBottom:!0,children:"Once we had our trained models, we used them to perturb images by calculating the gradient at the input level and adding it to an input perturbation vector. The input-level gradient allows us to note which features would lead the model to label images the way they did, so we use this to ideally perturb images so that these important features wouldn't show up. Finally, we saved the perturbed images generated from each model to create both a perturbed training and testing dataset."}),Object(j.jsx)(l.a,{gutterBottom:!0,children:"After generating our perturbed image datasets, we tested them on our architectures. For each architecture, we trained models in two different ways and compared each trained model\u2019s performance on the original and perturbed image datasets. The first way we trained each model type was with 50 epochs on the original CIFAR training data. The second way we trained each model type was with 25 epochs on the combined (original CIFAR and perturbed) training data. These variants were tested on the perturbed testing data and the original testing data. Finally, we compared the difference in accuracy of these two models across architecture groups."})]}),Object(j.jsxs)(u.a,{className:e.detailtext,children:[Object(j.jsx)(l.a,{variant:"h5",style:{textAlign:"center"},gutterBottom:!0,children:"Data Used"}),Object(j.jsx)(l.a,{gutterBottom:!0,children:"The main dataset that we used was CIFAR-10."}),Object(j.jsx)(l.a,{gutterBottom:!0,children:"The other datasets we used were those of perturbed images generated during the experiment, which were derived from CIFAR-10."})]}),Object(j.jsxs)(u.a,{className:e.detailtext,children:[Object(j.jsx)(l.a,{variant:"h5",style:{textAlign:"center"},gutterBottom:!0,children:"Techniques Used"}),Object(j.jsxs)(l.a,{gutterBottom:!0,children:["We used PyTorch for constructing our models. We also tuned our ResNet model from the pretrained model PyTorch provides. Information on the ResNet model can be found ",Object(j.jsx)("a",{href:"https://pytorch.org/vision/stable/models.html",children:"here"}),"."]}),Object(j.jsx)(l.a,{gutterBottom:!0,children:"Other than those, our model architecture and perturbation code was written ourselves."})]}),Object(j.jsxs)(u.a,{className:e.detailtext,children:[Object(j.jsx)(l.a,{variant:"h5",style:{textAlign:"center"},gutterBottom:!0,children:"Experiments and Results"}),Object(j.jsx)(l.a,{gutterBottom:!0,children:"TODO write description of the actual comparisons we made"}),Object(j.jsx)(l.a,{gutterBottom:!0,children:"TODO summarize our results"})]})]}),Object(j.jsx)(u.a,{className:"".concat(e.panel," ").concat(e.shinypanel),children:Object(j.jsx)(c.a,{style:{width:"40%"},children:Object(j.jsx)(b.a,{src:n,alt:"Leo DiCaprio numpy meme (credits: Will Chen)",color:"transparent",onClick:function(e){i(n===x?w:x)},style:{height:"100px"}})})}),Object(j.jsxs)(u.a,{className:e.panel,children:[Object(j.jsx)(l.a,{variant:"h4",style:{textAlign:"center"},gutterBottom:!0,children:"Discussion"}),Object(j.jsxs)(u.a,{className:e.detailtext,children:[Object(j.jsx)(l.a,{variant:"h5",style:{textAlign:"center"},gutterBottom:!0,children:"Problems Encountered"}),Object(j.jsx)(l.a,{gutterBottom:!0,children:"The first problem that we encountered encouraged a shift in our overall project approach. We had originally planned on using a GAN to generate adversarial perturbed images to attack our model, but when we tried this, we noticed how the GAN would create adversarial datasets containing images that don't match their label. Resolving this issue would involve a much larger code base than we believed we could develop, so we had to shift our project to a whitebox adversarial attack without the use of GAN's."}),Object(j.jsx)(l.a,{gutterBottom:!0,children:"Other than this, most problems we encountered were small bugs or difficulties with PyTorch and Google Colab. In particular, we repeatedly had issues with the models not training at all or having frustratingly long runtimes for training."})]}),Object(j.jsxs)(u.a,{className:e.detailtext,children:[Object(j.jsx)(l.a,{variant:"h5",style:{textAlign:"center"},gutterBottom:!0,children:"Next Steps"}),Object(j.jsx)(l.a,{gutterBottom:!0,children:"If we had more computational resources, we would explore more neural network architectures and see how those are affected by our adversarial attack. Alternatively, we could build on our current analysis by using visualizations to try to understand what accounts for the difference in impacts of image perturbation."}),Object(j.jsx)(l.a,{gutterBottom:!0,children:"We would also be interested in further exploring the idea of using GAN's and performing a black box adversarial attack."}),Object(j.jsx)(l.a,{gutterBottom:!0,children:"Finally, we would be interested in evaluating how well different neural network architectures respond to a more closed-loop GAN-based model training process."})]}),Object(j.jsxs)(u.a,{className:e.detailtext,children:[Object(j.jsx)(l.a,{variant:"h5",style:{textAlign:"center"},gutterBottom:!0,children:"How our approach differs from others"}),Object(j.jsx)(l.a,{gutterBottom:!0}),Object(j.jsx)(l.a,{gutterBottom:!0})]})]})]})}var O=function(e){e&&e instanceof Function&&a.e(3).then(a.bind(null,209)).then((function(t){var a=t.getCLS,r=t.getFID,n=t.getFCP,i=t.getLCP,o=t.getTTFB;a(e),r(e),n(e),i(e),o(e)}))};o.a.render(Object(j.jsx)(n.a.StrictMode,{children:Object(j.jsx)(y,{})}),document.getElementById("root")),O()},34:function(e,t,a){"use strict";(function(e){a.d(t,"c",(function(){return l})),a.d(t,"b",(function(){return u})),a.d(t,"e",(function(){return h})),a.d(t,"d",(function(){return p})),a.d(t,"a",(function(){return f}));var r=a(15),n=a.n(r),i=a(33),o=a(84),s=a.n(o),c=a(67),d=(a(68),a(20),"https://github.com/davidpfahler/react-ml-app/raw/master/src/dogs-resnet18.onnx"),l=function(e){return e.split("_").map((function(e){return e.charAt(0).toUpperCase()+e.slice(1)})).join(" ")},u=function(e){return"https://i.redd.it/vb4uq6nipk251.jpg"},h=function(){return new c.InferenceSession({backendHint:"webgl"})};function m(e){return g.apply(this,arguments)}function g(){return(g=Object(i.a)(n.a.mark((function e(t){return n.a.wrap((function(e){for(;;)switch(e.prev=e.next){case 0:case"end":return e.stop()}}),e)})))).apply(this,arguments)}function p(e){return b.apply(this,arguments)}function b(){return(b=Object(i.a)(n.a.mark((function e(t){return n.a.wrap((function(e){for(;;)switch(e.prev=e.next){case 0:return e.next=2,t.loadModel(d);case 2:return e.next=4,m(t);case 4:case"end":return e.stop()}}),e)})))).apply(this,arguments)}var x=function(t){return new Promise((function(a,r){e.setTimeout((function(){return a()}),t)}))},w={maxWidth:299,maxHeight:299,cover:!0,crop:!0,canvas:!0,crossOrigin:"Anonymous",orientation:!0},j=function(e){return new Promise((function(t,a){s()(e,(function(e){return t(e)}),w)}))},f=function(){var e=Object(i.a)(n.a.mark((function e(t,a,r){var i,o,s;return n.a.wrap((function(e){for(;;)switch(e.prev=e.next){case 0:if(a&&a.current){e.next=2;break}return e.abrupt("return");case 2:return e.next=4,j(t);case 4:if("error"!==(i=e.sent).type){e.next=7;break}throw new Error("could not load image");case 7:return(o=a.current.getContext("2d")).drawImage(i,0,0),e.next=11,x(1);case 11:s=o.getImageData(0,0,a.current.width,a.current.height),console.log("in fetchImage,"),console.log(a.current.width+" "+a.current.height),console.log(s),r(s);case 16:case"end":return e.stop()}}),e)})));return function(t,a,r){return e.apply(this,arguments)}}()}).call(this,a(132))}},[[155,1,2]]]);
//# sourceMappingURL=main.43a28392.chunk.js.map